\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\bibstyle{biblatex}
\bibdata{thesis-blx,bibliography/1cv,bibliography/1rl,bibliography/intro,bibliography/itw,bibliography/disc,bibliography/rlcoh,bibliography/VANS,bibliography/quantum,bibliography/quantera,bibliography/statinf,publications}
\citation{biblatex-control}
\abx@aux@refcontext{none/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@input{Editorial/title_small.aux}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@input{Editorial/left_blank.aux}
\@input{Editorial/title_page.aux}
\@input{Editorial/copyright.aux}
\@input{Editorial/dedication.aux}
\@input{Editorial/left_blank.aux}
\@input{Editorial/abstract_english.aux}
\@input{Editorial/abstract_catalan.aux}
\@input{Editorial/abstract_spanish.aux}
\@input{Editorial/left_blank.aux}
\@input{Editorial/declaration.aux}
\abx@aux@refsection{1}{7}
\abx@aux@cite{1}{bilkis2020realtime}
\abx@aux@segm{1}{0}{bilkis2020realtime}
\abx@aux@cite{1}{bilkis2021semi}
\abx@aux@segm{1}{0}{bilkis2021semi}
\abx@aux@cite{1}{bilkisitw}
\abx@aux@segm{1}{0}{bilkisitw}
\abx@aux@cite{1}{gasbarri2023sequential}
\abx@aux@segm{1}{0}{gasbarri2023sequential}
\abx@aux@cite{1}{bilkis2023machine}
\abx@aux@segm{1}{0}{bilkis2023machine}
\newlabel{refsection:1}{{}{vii}{Related works}{chapter*.5}{}}
\@input{Editorial/left_blank.aux}
\@input{Editorial/acknowledgments.aux}
\@input{Editorial/left_blank.aux}
\@input{Editorial/contents.aux}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Motivation}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminaries}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Some basic notions of quantum information}{5}{section.2.1}\protected@file@percent }
\citation{nielsen00}
\abx@aux@cite{0}{nielsen00}
\abx@aux@segm{0}{0}{nielsen00}
\newlabel{eq:}{{2.1}{6}{Some basic notions of quantum information}{equation.2.1.1}{}}
\newlabel{eq:}{{2.2}{6}{Some basic notions of quantum information}{equation.2.1.2}{}}
\newlabel{eq:}{{2.3}{6}{Some basic notions of quantum information}{equation.2.1.3}{}}
\newlabel{eq:}{{2.4}{6}{Some basic notions of quantum information}{equation.2.1.4}{}}
\newlabel{eq:}{{2.5}{7}{Some basic notions of quantum information}{equation.2.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Projective measurements}{7}{subsubsection*.8}\protected@file@percent }
\newlabel{eq:}{{2.6}{7}{Projective measurements}{equation.2.1.6}{}}
\newlabel{eq:}{{2.7}{7}{Projective measurements}{equation.2.1.7}{}}
\newlabel{eq:}{{2.8}{7}{Projective measurements}{equation.2.1.8}{}}
\newlabel{eq:}{{2.9}{8}{Projective measurements}{equation.2.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Closed-system dynamics}{8}{subsection.2.1.1}\protected@file@percent }
\newlabel{ssec:1_intro_unitary_evo}{{2.1.1}{8}{Closed-system dynamics}{subsection.2.1.1}{}}
\newlabel{eq:}{{2.10}{8}{Closed-system dynamics}{equation.2.1.10}{}}
\newlabel{eq:schro}{{2.11}{8}{Closed-system dynamics}{equation.2.1.11}{}}
\newlabel{eq:}{{2.12}{8}{Closed-system dynamics}{equation.2.1.12}{}}
\newlabel{eq:}{{2.13}{8}{Closed-system dynamics}{equation.2.1.13}{}}
\newlabel{eq:}{{2.14}{8}{Closed-system dynamics}{equation.2.1.14}{}}
\citation{manzano2019}
\abx@aux@cite{0}{manzano2019}
\abx@aux@segm{0}{0}{manzano2019}
\newlabel{eq:}{{2.15}{9}{Closed-system dynamics}{equation.2.1.15}{}}
\newlabel{eq:}{{2.16}{9}{Closed-system dynamics}{equation.2.1.16}{}}
\newlabel{eq:}{{2.17}{9}{Closed-system dynamics}{equation.2.1.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Quantum channels}{9}{subsection.2.1.2}\protected@file@percent }
\newlabel{ssec:1_intro_channels}{{2.1.2}{9}{Quantum channels}{subsection.2.1.2}{}}
\newlabel{eq:}{{2.18}{9}{Quantum channels}{equation.2.1.18}{}}
\citation{manzano2019}
\abx@aux@cite{0}{manzano2019}
\abx@aux@segm{0}{0}{manzano2019}
\newlabel{eq:compKraus}{{2.20}{10}{Quantum channels}{equation.2.1.20}{}}
\newlabel{eq:stinesrping}{{2.21}{10}{Quantum channels}{equation.2.1.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Generalized quantum measurements}{10}{subsection.2.1.3}\protected@file@percent }
\newlabel{ssec:1_intro_qmeas}{{2.1.3}{10}{Generalized quantum measurements}{subsection.2.1.3}{}}
\newlabel{eq:bornrule}{{2.24}{10}{Generalized quantum measurements}{equation.2.1.24}{}}
\citation{watrous_2018}
\abx@aux@cite{0}{watrous_2018}
\abx@aux@segm{0}{0}{watrous_2018}
\citation{watrous_2018}
\abx@aux@cite{0}{watrous_2018}
\abx@aux@segm{0}{0}{watrous_2018}
\citation{Preskill1998}
\abx@aux@cite{0}{Preskill1998}
\abx@aux@segm{0}{0}{Preskill1998}
\newlabel{eq:}{{2.25}{11}{Generalized quantum measurements}{equation.2.1.25}{}}
\newlabel{eq:}{{2.26}{11}{Generalized quantum measurements}{equation.2.1.26}{}}
\newlabel{eq:measurementdynamics}{{2.27}{11}{Generalized quantum measurements}{equation.2.1.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Quantum instruments}{11}{subsection.2.1.4}\protected@file@percent }
\newlabel{ssec:1_intro_operations}{{2.1.4}{11}{Quantum instruments}{subsection.2.1.4}{}}
\citation{libland1976}
\abx@aux@cite{0}{libland1976}
\abx@aux@segm{0}{0}{libland1976}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\citation{manzano2019}
\abx@aux@cite{0}{manzano2019}
\abx@aux@segm{0}{0}{manzano2019}
\newlabel{eq:}{{2.28}{12}{Quantum instruments}{equation.2.1.28}{}}
\newlabel{eq:}{{2.29}{12}{Quantum instruments}{equation.2.1.29}{}}
\newlabel{eq:}{{2.30}{12}{Quantum instruments}{equation.2.1.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Quantum master equation}{12}{subsection.2.1.5}\protected@file@percent }
\newlabel{ssec:1_intro_open}{{2.1.5}{12}{Quantum master equation}{subsection.2.1.5}{}}
\newlabel{eq:}{{2.31}{13}{Quantum master equation}{equation.2.1.31}{}}
\newlabel{eq:libland}{{2.32}{13}{Quantum master equation}{equation.2.1.32}{}}
\newlabel{eq:invalibland}{{2.33}{13}{Quantum master equation}{equation.2.1.33}{}}
\newlabel{eq:ravel}{{2.34}{13}{Quantum master equation}{equation.2.1.34}{}}
\citation{preskill2018quantum}
\abx@aux@cite{0}{preskill2018quantum}
\abx@aux@segm{0}{0}{preskill2018quantum}
\citation{Fowler2012surface}
\abx@aux@cite{0}{Fowler2012surface}
\abx@aux@segm{0}{0}{Fowler2012surface}
\citation{supremacygoogle}
\abx@aux@cite{0}{supremacygoogle}
\abx@aux@segm{0}{0}{supremacygoogle}
\citation{nosupremacy}
\abx@aux@cite{0}{nosupremacy}
\abx@aux@segm{0}{0}{nosupremacy}
\citation{chemistrynono}
\abx@aux@cite{0}{chemistrynono}
\abx@aux@segm{0}{0}{chemistrynono}
\citation{useful2discoverQC}
\abx@aux@cite{0}{useful2discoverQC}
\abx@aux@segm{0}{0}{useful2discoverQC}
\citation{mcclean2018barren}
\abx@aux@cite{0}{mcclean2018barren}
\abx@aux@segm{0}{0}{mcclean2018barren}
\citation{cerezo2020cost}
\abx@aux@cite{0}{cerezo2020cost}
\abx@aux@segm{0}{0}{cerezo2020cost}
\citation{pesah2020absence}
\abx@aux@cite{0}{pesah2020absence}
\abx@aux@segm{0}{0}{pesah2020absence}
\citation{holmes2020barren}
\abx@aux@cite{0}{holmes2020barren}
\abx@aux@segm{0}{0}{holmes2020barren}
\citation{zhao2021analyzing}
\abx@aux@cite{0}{zhao2021analyzing}
\abx@aux@segm{0}{0}{zhao2021analyzing}
\citation{thanasilp2021subtleties}
\abx@aux@cite{0}{thanasilp2021subtleties}
\abx@aux@segm{0}{0}{thanasilp2021subtleties}
\citation{sharma2020trainability}
\abx@aux@cite{0}{sharma2020trainability}
\abx@aux@segm{0}{0}{sharma2020trainability}
\citation{patti2020entanglement}
\abx@aux@cite{0}{patti2020entanglement}
\abx@aux@segm{0}{0}{patti2020entanglement}
\citation{marrero2020entanglement}
\abx@aux@cite{0}{marrero2020entanglement}
\abx@aux@segm{0}{0}{marrero2020entanglement}
\citation{cerezo2020impact}
\abx@aux@cite{0}{cerezo2020impact}
\abx@aux@segm{0}{0}{cerezo2020impact}
\citation{arrasmith2020effect}
\abx@aux@cite{0}{arrasmith2020effect}
\abx@aux@segm{0}{0}{arrasmith2020effect}
\citation{npHardVQA}
\abx@aux@cite{0}{npHardVQA}
\abx@aux@segm{0}{0}{npHardVQA}
\citation{biamonte2017quantum}
\abx@aux@cite{0}{biamonte2017quantum}
\abx@aux@segm{0}{0}{biamonte2017quantum}
\citation{supremacygoogle}
\abx@aux@cite{0}{supremacygoogle}
\abx@aux@segm{0}{0}{supremacygoogle}
\citation{schuld2021isquantum}
\abx@aux@cite{0}{schuld2021isquantum}
\abx@aux@segm{0}{0}{schuld2021isquantum}
\citation{Aaronson2015}
\abx@aux@cite{0}{Aaronson2015}
\abx@aux@segm{0}{0}{Aaronson2015}
\citation{preskill2018quantum}
\abx@aux@cite{0}{preskill2018quantum}
\abx@aux@segm{0}{0}{preskill2018quantum}
\citation{peruzzo2014variational}
\abx@aux@cite{0}{peruzzo2014variational}
\abx@aux@segm{0}{0}{peruzzo2014variational}
\citation{pesah2021absense}
\abx@aux@cite{0}{pesah2021absense}
\abx@aux@segm{0}{0}{pesah2021absense}
\citation{sharma2022reformulation}
\abx@aux@cite{0}{sharma2022reformulation}
\abx@aux@segm{0}{0}{sharma2022reformulation}
\citation{Goodfellow-et-al-2016}
\abx@aux@cite{0}{Goodfellow-et-al-2016}
\abx@aux@segm{0}{0}{Goodfellow-et-al-2016}
\citation{VQA_revmarco}
\abx@aux@cite{0}{VQA_revmarco}
\abx@aux@segm{0}{0}{VQA_revmarco}
\citation{VQA_revalba}
\abx@aux@cite{0}{VQA_revalba}
\abx@aux@segm{0}{0}{VQA_revalba}
\newlabel{sec:1_introquantum}{{2.1.5}{14}{Quantum master equation}{equation.2.1.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}NISQ $\&$ the qubits}{14}{section.2.2}\protected@file@percent }
\newlabel{sec:1_nisq}{{2.2}{14}{NISQ $\&$ the qubits}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Variational Quantum algorithms}{14}{subsection.2.2.1}\protected@file@percent }
\newlabel{ssec:1_nisq_vqa}{{2.2.1}{14}{Variational Quantum algorithms}{subsection.2.2.1}{}}
\citation{peruzzo2014variational}
\abx@aux@cite{0}{peruzzo2014variational}
\abx@aux@segm{0}{0}{peruzzo2014variational}
\citation{classiVQA}
\abx@aux@cite{0}{classiVQA}
\abx@aux@segm{0}{0}{classiVQA}
\citation{vqaclass1}
\abx@aux@cite{0}{vqaclass1}
\abx@aux@segm{0}{0}{vqaclass1}
\citation{vqaclass2}
\abx@aux@cite{0}{vqaclass2}
\abx@aux@segm{0}{0}{vqaclass2}
\citation{vqaclass4}
\abx@aux@cite{0}{vqaclass4}
\abx@aux@segm{0}{0}{vqaclass4}
\citation{cong2019quantum}
\abx@aux@cite{0}{cong2019quantum}
\abx@aux@segm{0}{0}{cong2019quantum}
\newlabel{eq:cost}{{2.36}{15}{Variational Quantum algorithms}{equation.2.2.36}{}}
\newlabel{eq:vqe_cost}{{2.37}{15}{Variational Quantum algorithms}{equation.2.2.37}{}}
\citation{elben2022randomized}
\abx@aux@cite{0}{elben2022randomized}
\abx@aux@segm{0}{0}{elben2022randomized}
\citation{borrowed}
\abx@aux@cite{0}{borrowed}
\abx@aux@segm{0}{0}{borrowed}
\citation{borrowed}
\abx@aux@cite{0}{borrowed}
\abx@aux@segm{0}{0}{borrowed}
\citation{verdon2018universal}
\abx@aux@cite{0}{verdon2018universal}
\abx@aux@segm{0}{0}{verdon2018universal}
\citation{kubler2020adaptive}
\abx@aux@cite{0}{kubler2020adaptive}
\abx@aux@segm{0}{0}{kubler2020adaptive}
\citation{arrasmith2020operator}
\abx@aux@cite{0}{arrasmith2020operator}
\abx@aux@segm{0}{0}{arrasmith2020operator}
\citation{stokes2020quantum}
\abx@aux@cite{0}{stokes2020quantum}
\abx@aux@segm{0}{0}{stokes2020quantum}
\citation{koczor2019quantum}
\abx@aux@cite{0}{koczor2019quantum}
\abx@aux@segm{0}{0}{koczor2019quantum}
\citation{nakanishi2020sequential}
\abx@aux@cite{0}{nakanishi2020sequential}
\abx@aux@segm{0}{0}{nakanishi2020sequential}
\citation{fontana2020optimizing}
\abx@aux@cite{0}{fontana2020optimizing}
\abx@aux@segm{0}{0}{fontana2020optimizing}
\newlabel{eq:decoH}{{2.38}{16}{Variational Quantum algorithms}{equation.2.2.38}{}}
\newlabel{eq:optimization}{{2.39}{16}{Variational Quantum algorithms}{equation.2.2.39}{}}
\citation{holmes2021connecting}
\abx@aux@cite{0}{holmes2021connecting}
\abx@aux@segm{0}{0}{holmes2021connecting}
\citation{nielsen00}
\abx@aux@cite{0}{nielsen00}
\abx@aux@segm{0}{0}{nielsen00}
\citation{compilingDeepRLmaster}
\abx@aux@cite{0}{compilingDeepRLmaster}
\abx@aux@segm{0}{0}{compilingDeepRLmaster}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Figure adapted from Ref.~\blx@tocontentsinit {0}\cite {borrowed}: a generic VQA algortihm is sketched. Here, a quantum device is used to estimate a cost-function value, whereas the device itself is controlled by a classical optimization algorithm. This cycle is iterated until convergence.\relax }}{17}{figure.caption.9}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:VQAcycle}{{2.1}{17}{Figure adapted from Ref.~\cite {borrowed}: a generic VQA algortihm is sketched. Here, a quantum device is used to estimate a cost-function value, whereas the device itself is controlled by a classical optimization algorithm. This cycle is iterated until convergence.\relax }{figure.caption.9}{}}
\citation{wikiansa}
\abx@aux@cite{0}{wikiansa}
\abx@aux@segm{0}{0}{wikiansa}
\citation{separableAnna}
\abx@aux@cite{0}{separableAnna}
\abx@aux@segm{0}{0}{separableAnna}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The Ansatz}{18}{subsection.2.2.2}\protected@file@percent }
\newlabel{ssec:1_nisq_vans_bp}{{2.2.2}{18}{The Ansatz}{subsection.2.2.2}{}}
\citation{kandala2017hardware}
\abx@aux@cite{0}{kandala2017hardware}
\abx@aux@segm{0}{0}{kandala2017hardware}
\citation{holmes2021connecting}
\abx@aux@cite{0}{holmes2021connecting}
\abx@aux@segm{0}{0}{holmes2021connecting}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces We show examples of commonly-used quantum circuits. In \textit  {(a)} we depict a separable product ansatz which generates no entanglement between the qubits. On the other hand, \textit  {(b)} shows two layers of a shallow alternating Hardware Efficient Ansatz where neighboring qubits are initially entangled. Here $Z$ ($X$) indicates a parametrized rotation about the $z$ ($x$) axis (the angles $\bm  {\theta }$ are not explicitely shown, but such are the degree of freedom to optimize over).\relax }}{19}{figure.caption.10}\protected@file@percent }
\newlabel{fig:FANSATZ}{{2.2}{19}{We show examples of commonly-used quantum circuits. In \textit {(a)} we depict a separable product ansatz which generates no entanglement between the qubits. On the other hand, \textit {(b)} shows two layers of a shallow alternating Hardware Efficient Ansatz where neighboring qubits are initially entangled. Here $Z$ ($X$) indicates a parametrized rotation about the $z$ ($x$) axis (the angles $\bm {\theta }$ are not explicitely shown, but such are the degree of freedom to optimize over).\relax }{figure.caption.10}{}}
\citation{farhi2014quantum}
\abx@aux@cite{0}{farhi2014quantum}
\abx@aux@segm{0}{0}{farhi2014quantum}
\citation{hadfield2019quantum}
\abx@aux@cite{0}{hadfield2019quantum}
\abx@aux@segm{0}{0}{hadfield2019quantum}
\citation{cao2019quantum}
\abx@aux@cite{0}{cao2019quantum}
\abx@aux@segm{0}{0}{cao2019quantum}
\citation{bartlett2007coupled}
\abx@aux@cite{0}{bartlett2007coupled}
\abx@aux@segm{0}{0}{bartlett2007coupled}
\citation{grimsley2019adaptive}
\abx@aux@cite{0}{grimsley2019adaptive}
\abx@aux@segm{0}{0}{grimsley2019adaptive}
\citation{tang2019qubit}
\abx@aux@cite{0}{tang2019qubit}
\abx@aux@segm{0}{0}{tang2019qubit}
\citation{zhang2021mutual}
\abx@aux@cite{0}{zhang2021mutual}
\abx@aux@segm{0}{0}{zhang2021mutual}
\citation{rattew2019domain}
\abx@aux@cite{0}{rattew2019domain}
\abx@aux@segm{0}{0}{rattew2019domain}
\citation{chivilikhin2020mog}
\abx@aux@cite{0}{chivilikhin2020mog}
\abx@aux@segm{0}{0}{chivilikhin2020mog}
\citation{cincio2021machine}
\abx@aux@cite{0}{cincio2021machine}
\abx@aux@segm{0}{0}{cincio2021machine}
\citation{cincio2018learning}
\abx@aux@cite{0}{cincio2018learning}
\abx@aux@segm{0}{0}{cincio2018learning}
\citation{du2020quantum}
\abx@aux@cite{0}{du2020quantum}
\abx@aux@segm{0}{0}{du2020quantum}
\citation{zhang2020differentiable}
\abx@aux@cite{0}{zhang2020differentiable}
\abx@aux@segm{0}{0}{zhang2020differentiable}
\citation{algorithmiq1}
\abx@aux@cite{0}{algorithmiq1}
\abx@aux@segm{0}{0}{algorithmiq1}
\citation{algorithmiq2}
\abx@aux@cite{0}{algorithmiq2}
\abx@aux@segm{0}{0}{algorithmiq2}
\citation{grimsley2019adaptive}
\abx@aux@cite{0}{grimsley2019adaptive}
\abx@aux@segm{0}{0}{grimsley2019adaptive}
\citation{tang2019qubit}
\abx@aux@cite{0}{tang2019qubit}
\abx@aux@segm{0}{0}{tang2019qubit}
\citation{claudino2020benchmarking}
\abx@aux@cite{0}{claudino2020benchmarking}
\abx@aux@segm{0}{0}{claudino2020benchmarking}
\citation{rattew2019domain}
\abx@aux@cite{0}{rattew2019domain}
\abx@aux@segm{0}{0}{rattew2019domain}
\citation{chivilikhin2020mog}
\abx@aux@cite{0}{chivilikhin2020mog}
\abx@aux@segm{0}{0}{chivilikhin2020mog}
\citation{du2020quantum}
\abx@aux@cite{0}{du2020quantum}
\abx@aux@segm{0}{0}{du2020quantum}
\citation{zhang2020differentiable}
\abx@aux@cite{0}{zhang2020differentiable}
\abx@aux@segm{0}{0}{zhang2020differentiable}
\citation{pirhooshyaran2021quantum}
\abx@aux@cite{0}{pirhooshyaran2021quantum}
\abx@aux@segm{0}{0}{pirhooshyaran2021quantum}
\citation{du2020quantum}
\abx@aux@cite{0}{du2020quantum}
\abx@aux@segm{0}{0}{du2020quantum}
\citation{cincio2018learning}
\abx@aux@cite{0}{cincio2018learning}
\abx@aux@segm{0}{0}{cincio2018learning}
\citation{cincio2021machine}
\abx@aux@cite{0}{cincio2021machine}
\abx@aux@segm{0}{0}{cincio2021machine}
\citation{bilkis2021semi}
\abx@aux@cite{0}{bilkis2021semi}
\abx@aux@segm{0}{0}{bilkis2021semi}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}The optimization procedure}{22}{subsection.2.2.3}\protected@file@percent }
\newlabel{ssec:optimizer}{{2.2.3}{22}{The optimization procedure}{subsection.2.2.3}{}}
\citation{kingma2015adam}
\abx@aux@cite{0}{kingma2015adam}
\abx@aux@segm{0}{0}{kingma2015adam}
\citation{verdon2018universal}
\abx@aux@cite{0}{verdon2018universal}
\abx@aux@segm{0}{0}{verdon2018universal}
\citation{kubler2020adaptive}
\abx@aux@cite{0}{kubler2020adaptive}
\abx@aux@segm{0}{0}{kubler2020adaptive}
\citation{arrasmith2020operator}
\abx@aux@cite{0}{arrasmith2020operator}
\abx@aux@segm{0}{0}{arrasmith2020operator}
\citation{stokes2020quantum}
\abx@aux@cite{0}{stokes2020quantum}
\abx@aux@segm{0}{0}{stokes2020quantum}
\citation{koczor2019quantum}
\abx@aux@cite{0}{koczor2019quantum}
\abx@aux@segm{0}{0}{koczor2019quantum}
\citation{nakanishi2020sequential}
\abx@aux@cite{0}{nakanishi2020sequential}
\abx@aux@segm{0}{0}{nakanishi2020sequential}
\citation{fontana2020optimizing}
\abx@aux@cite{0}{fontana2020optimizing}
\abx@aux@segm{0}{0}{fontana2020optimizing}
\citation{gu2021adaptive}
\abx@aux@cite{0}{gu2021adaptive}
\abx@aux@segm{0}{0}{gu2021adaptive}
\newlabel{eq:GD}{{2.40}{23}{The optimization procedure}{equation.2.2.40}{}}
\citation{Biamonte2017tensornetworks}
\abx@aux@cite{0}{Biamonte2017tensornetworks}
\abx@aux@segm{0}{0}{Biamonte2017tensornetworks}
\citation{orusTN}
\abx@aux@cite{0}{orusTN}
\abx@aux@segm{0}{0}{orusTN}
\citation{broughton2020tensorflow}
\abx@aux@cite{0}{broughton2020tensorflow}
\abx@aux@segm{0}{0}{broughton2020tensorflow}
\citation{Luo2020yaojlextensible}
\abx@aux@cite{0}{Luo2020yaojlextensible}
\abx@aux@segm{0}{0}{Luo2020yaojlextensible}
\citation{Rackauckas2020GeneralizedPL}
\abx@aux@cite{0}{Rackauckas2020GeneralizedPL}
\abx@aux@segm{0}{0}{Rackauckas2020GeneralizedPL}
\citation{Liao2019differentaible}
\abx@aux@cite{0}{Liao2019differentaible}
\abx@aux@segm{0}{0}{Liao2019differentaible}
\citation{dpcontrolsto}
\abx@aux@cite{0}{dpcontrolsto}
\abx@aux@segm{0}{0}{dpcontrolsto}
\citation{wikiAD}
\abx@aux@cite{0}{wikiAD}
\abx@aux@segm{0}{0}{wikiAD}
\@writefile{toc}{\contentsline {subsubsection}{Classical simulation of quantum circuits $\&$ Automatic differentiation}{24}{subsubsection*.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces We show how the computational graph is travelled forward and backwards in each of the possible modes of AD, for the example of an scalar function $f(x_1,x_2) = x_1 x_2 + \qopname  \relax o{sin}(x_1)$\relax }}{25}{figure.caption.12}\protected@file@percent }
\newlabel{fig:ad}{{2.3}{25}{We show how the computational graph is travelled forward and backwards in each of the possible modes of AD, for the example of an scalar function $f(x_1,x_2) = x_1 x_2 + \sin (x_1)$\relax }{figure.caption.12}{}}
\newlabel{eq:}{{2.41}{25}{Classical simulation of quantum circuits $\&$ Automatic differentiation}{equation.2.2.41}{}}
\citation{wikiAD}
\abx@aux@cite{0}{wikiAD}
\abx@aux@segm{0}{0}{wikiAD}
\citation{abadi2016tensorflow}
\abx@aux@cite{0}{abadi2016tensorflow}
\abx@aux@segm{0}{0}{abadi2016tensorflow}
\citation{maclaurinautograd}
\abx@aux@cite{0}{maclaurinautograd}
\abx@aux@segm{0}{0}{maclaurinautograd}
\citation{maclaurin2016phd}
\abx@aux@cite{0}{maclaurin2016phd}
\abx@aux@segm{0}{0}{maclaurin2016phd}
\citation{survey_ad}
\abx@aux@cite{0}{survey_ad}
\abx@aux@segm{0}{0}{survey_ad}
\@writefile{toc}{\contentsline {subsubsection}{Experimental implementations on quantum hardware}{26}{subsubsection*.13}\protected@file@percent }
\citation{shiftrules1}
\abx@aux@cite{0}{shiftrules1}
\abx@aux@segm{0}{0}{shiftrules1}
\citation{shiftrules1}
\abx@aux@cite{0}{shiftrules1}
\abx@aux@segm{0}{0}{shiftrules1}
\citation{shiftrules2}
\abx@aux@cite{0}{shiftrules2}
\abx@aux@segm{0}{0}{shiftrules2}
\citation{Wierichs2022generalparameter}
\abx@aux@cite{0}{Wierichs2022generalparameter}
\abx@aux@segm{0}{0}{Wierichs2022generalparameter}
\citation{Sweke2020}
\abx@aux@cite{0}{Sweke2020}
\abx@aux@segm{0}{0}{Sweke2020}
\citation{Sweke2020}
\abx@aux@cite{0}{Sweke2020}
\abx@aux@segm{0}{0}{Sweke2020}
\citation{harrow2019low}
\abx@aux@cite{0}{harrow2019low}
\abx@aux@segm{0}{0}{harrow2019low}
\newlabel{eq:shifted}{{2.48}{27}{Experimental implementations on quantum hardware}{equation.2.2.48}{}}
\newlabel{eq:}{{2.50}{27}{Experimental implementations on quantum hardware}{equation.2.2.50}{}}
\newlabel{eq:der}{{2.51}{27}{Experimental implementations on quantum hardware}{equation.2.2.51}{}}
\citation{bergholm2018pennylane}
\abx@aux@cite{0}{bergholm2018pennylane}
\abx@aux@segm{0}{0}{bergholm2018pennylane}
\citation{broughton2020tensorflow}
\abx@aux@cite{0}{broughton2020tensorflow}
\abx@aux@segm{0}{0}{broughton2020tensorflow}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Barren-plateaus}{28}{subsection.2.2.4}\protected@file@percent }
\newlabel{ssec:1_nisq_barrenplateaus}{{2.2.4}{28}{Barren-plateaus}{subsection.2.2.4}{}}
\citation{holmes2021connecting}
\abx@aux@cite{0}{holmes2021connecting}
\abx@aux@segm{0}{0}{holmes2021connecting}
\citation{mcclean2018barren}
\abx@aux@cite{0}{mcclean2018barren}
\abx@aux@segm{0}{0}{mcclean2018barren}
\newlabel{eq:}{{2.52}{29}{Barren-plateaus}{equation.2.2.52}{}}
\newlabel{eq:relations_haar}{{2.53}{29}{Barren-plateaus}{equation.2.2.53}{}}
\citation{mcclean2018barren}
\abx@aux@cite{0}{mcclean2018barren}
\abx@aux@segm{0}{0}{mcclean2018barren}
\citation{cerezo2020cost}
\abx@aux@cite{0}{cerezo2020cost}
\abx@aux@segm{0}{0}{cerezo2020cost}
\citation{sharma2020trainability}
\abx@aux@cite{0}{sharma2020trainability}
\abx@aux@segm{0}{0}{sharma2020trainability}
\citation{holmes2021connecting}
\abx@aux@cite{0}{holmes2021connecting}
\abx@aux@segm{0}{0}{holmes2021connecting}
\citation{larocca2021diagnosing}
\abx@aux@cite{0}{larocca2021diagnosing}
\abx@aux@segm{0}{0}{larocca2021diagnosing}
\citation{sukin2019expressibility}
\abx@aux@cite{0}{sukin2019expressibility}
\abx@aux@segm{0}{0}{sukin2019expressibility}
\citation{sharma2020trainability}
\abx@aux@cite{0}{sharma2020trainability}
\abx@aux@segm{0}{0}{sharma2020trainability}
\citation{patti2020entanglement}
\abx@aux@cite{0}{patti2020entanglement}
\abx@aux@segm{0}{0}{patti2020entanglement}
\citation{marrero2020entanglement}
\abx@aux@cite{0}{marrero2020entanglement}
\abx@aux@segm{0}{0}{marrero2020entanglement}
\citation{volkoff2021large}
\abx@aux@cite{0}{volkoff2021large}
\abx@aux@segm{0}{0}{volkoff2021large}
\citation{skolik2020layerwise}
\abx@aux@cite{0}{skolik2020layerwise}
\abx@aux@segm{0}{0}{skolik2020layerwise}
\citation{grant2019initialization}
\abx@aux@cite{0}{grant2019initialization}
\abx@aux@segm{0}{0}{grant2019initialization}
\citation{verdon2019learning}
\abx@aux@cite{0}{verdon2019learning}
\abx@aux@segm{0}{0}{verdon2019learning}
\citation{Cong2019}
\abx@aux@cite{0}{Cong2019}
\abx@aux@segm{0}{0}{Cong2019}
\citation{pesah2020absence}
\abx@aux@cite{0}{pesah2020absence}
\abx@aux@segm{0}{0}{pesah2020absence}
\newlabel{eq:BP}{{2.55}{30}{Barren-plateaus}{equation.2.2.55}{}}
\newlabel{eq:Chebyshev}{{2.56}{30}{Barren-plateaus}{equation.2.2.56}{}}
\citation{wang2020noise}
\abx@aux@cite{0}{wang2020noise}
\abx@aux@segm{0}{0}{wang2020noise}
\citation{wang2020noise}
\abx@aux@cite{0}{wang2020noise}
\abx@aux@segm{0}{0}{wang2020noise}
\citation{franca2020limitations}
\abx@aux@cite{0}{franca2020limitations}
\abx@aux@segm{0}{0}{franca2020limitations}
\citation{arrasmith2020effect}
\abx@aux@cite{0}{arrasmith2020effect}
\abx@aux@segm{0}{0}{arrasmith2020effect}
\citation{verdon2019learning}
\abx@aux@cite{0}{verdon2019learning}
\abx@aux@segm{0}{0}{verdon2019learning}
\citation{volkoff2021large}
\abx@aux@cite{0}{volkoff2021large}
\abx@aux@segm{0}{0}{volkoff2021large}
\citation{skolik2020layerwise}
\abx@aux@cite{0}{skolik2020layerwise}
\abx@aux@segm{0}{0}{skolik2020layerwise}
\citation{grant2019initialization}
\abx@aux@cite{0}{grant2019initialization}
\abx@aux@segm{0}{0}{grant2019initialization}
\citation{pesah2020absence}
\abx@aux@cite{0}{pesah2020absence}
\abx@aux@segm{0}{0}{pesah2020absence}
\citation{zhang2020toward}
\abx@aux@cite{0}{zhang2020toward}
\abx@aux@segm{0}{0}{zhang2020toward}
\citation{bharti2020quantum}
\abx@aux@cite{0}{bharti2020quantum}
\abx@aux@segm{0}{0}{bharti2020quantum}
\citation{cerezo2020variational}
\abx@aux@cite{0}{cerezo2020variational}
\abx@aux@segm{0}{0}{cerezo2020variational}
\newlabel{eq:NIBP}{{2.57}{31}{Barren-plateaus}{equation.2.2.57}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Discussion}{31}{subsection.2.2.5}\protected@file@percent }
\newlabel{ssec:1_nisq_discu}{{2.2.5}{31}{Discussion}{subsection.2.2.5}{}}
\citation{serafiniBOOK}
\abx@aux@cite{0}{serafiniBOOK}
\abx@aux@segm{0}{0}{serafiniBOOK}
\citation{RevGauss}
\abx@aux@cite{0}{RevGauss}
\abx@aux@segm{0}{0}{RevGauss}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Continuous-variable systems}{32}{section.2.3}\protected@file@percent }
\newlabel{sec:1_cv}{{2.3}{32}{Continuous-variable systems}{section.2.3}{}}
\newlabel{eq:ccr}{{2.58}{32}{Continuous-variable systems}{equation.2.3.58}{}}
\newlabel{eq:cv_ccr}{{2.59}{33}{Continuous-variable systems}{equation.2.3.59}{}}
\newlabel{eq:}{{2.61}{33}{Continuous-variable systems}{equation.2.3.61}{}}
\newlabel{eq:compoweyl}{{2.64}{33}{Continuous-variable systems}{equation.2.3.64}{}}
\newlabel{eq:actionweylop}{{2.65}{33}{Continuous-variable systems}{equation.2.3.65}{}}
\newlabel{eq:campb}{{2.62}{33}{}{equation.2.3.62}{}}
\newlabel{eq:camsim}{{2.63}{33}{}{equation.2.3.63}{}}
\newlabel{eq:weyladag}{{2.66}{34}{Continuous-variable systems}{equation.2.3.66}{}}
\newlabel{eq:weylaop}{{2.68}{34}{Continuous-variable systems}{equation.2.3.68}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Gaussian systems}{34}{subsection.2.3.1}\protected@file@percent }
\newlabel{ssec:intro_cv_gaussianinfo}{{2.3.1}{34}{Gaussian systems}{subsection.2.3.1}{}}
\newlabel{eq:rhog}{{2.69}{34}{Gaussian systems}{equation.2.3.69}{}}
\newlabel{eq:hamiGauss}{{2.70}{34}{Gaussian systems}{equation.2.3.70}{}}
\newlabel{eq:h1}{{2.71}{34}{Gaussian systems}{equation.2.3.71}{}}
\newlabel{eq:hp}{{2.72}{34}{Gaussian systems}{equation.2.3.72}{}}
\citation{Reck1994Experimental}
\abx@aux@cite{0}{Reck1994Experimental}
\abx@aux@segm{0}{0}{Reck1994Experimental}
\citation{Weedbrook2012Gaussian}
\abx@aux@cite{0}{Weedbrook2012Gaussian}
\abx@aux@segm{0}{0}{Weedbrook2012Gaussian}
\newlabel{eq:pres}{{2.73}{35}{Gaussian systems}{equation.2.3.73}{}}
\newlabel{eq:symp}{{2.74}{35}{Gaussian systems}{equation.2.3.74}{}}
\newlabel{eq:00}{{2.75}{35}{Gaussian systems}{equation.2.3.75}{}}
\newlabel{eq:sympo}{{2.76}{35}{Gaussian systems}{equation.2.3.76}{}}
\newlabel{eq:svdS}{{2.77}{35}{Gaussian systems}{equation.2.3.77}{}}
\newlabel{eq:passive1}{{2.78}{35}{Gaussian systems}{equation.2.3.78}{}}
\citation{Williamson1936Algebraic}
\abx@aux@cite{0}{Williamson1936Algebraic}
\abx@aux@segm{0}{0}{Williamson1936Algebraic}
\citation{pereira2021symplectic}
\abx@aux@cite{0}{pereira2021symplectic}
\abx@aux@segm{0}{0}{pereira2021symplectic}
\citation{Pirandola2009correlation}
\abx@aux@cite{0}{Pirandola2009correlation}
\abx@aux@segm{0}{0}{Pirandola2009correlation}
\newlabel{eq:colhonbs}{{2.79}{36}{Gaussian systems}{equation.2.3.79}{}}
\newlabel{eq:}{{2.81}{36}{Gaussian systems}{equation.2.3.81}{}}
\newlabel{eq:1gaussrho}{{2.88}{37}{Gaussian systems}{equation.2.3.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Some methods in phase space}{37}{subsection.2.3.2}\protected@file@percent }
\newlabel{ssec:intro_cv_phase}{{2.3.2}{37}{Some methods in phase space}{subsection.2.3.2}{}}
\newlabel{eq:1moms}{{2.90}{37}{Some methods in phase space}{equation.2.3.90}{}}
\newlabel{eq:uncer}{{2.91}{38}{Some methods in phase space}{equation.2.3.91}{}}
\newlabel{eq:pure_cov_gauss}{{2.92}{38}{Some methods in phase space}{equation.2.3.92}{}}
\newlabel{eq:def_coherent}{{2.93}{39}{Some methods in phase space}{equation.2.3.93}{}}
\newlabel{eq:1fockcoherent}{{2.94}{39}{Some methods in phase space}{equation.2.3.94}{}}
\newlabel{eq:resocoh}{{2.95}{39}{Some methods in phase space}{equation.2.3.95}{}}
\newlabel{eq:trW}{{2.96}{39}{Some methods in phase space}{equation.2.3.96}{}}
\newlabel{eq:}{{2.98}{39}{Some methods in phase space}{equation.2.3.98}{}}
\citation{nongaussian}
\abx@aux@cite{0}{nongaussian}
\abx@aux@segm{0}{0}{nongaussian}
\citation{Weedbrook2012Gaussian}
\abx@aux@cite{0}{Weedbrook2012Gaussian}
\abx@aux@segm{0}{0}{Weedbrook2012Gaussian}
\citation{Olivares2012}
\abx@aux@cite{0}{Olivares2012}
\abx@aux@segm{0}{0}{Olivares2012}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces We show the Wigner functions of the vacuum state $\ket {0}$ (left) and of the Fock state $\ket {5}$ (right). While in the former case the probability distribution is a Gaussian, the latter represents a non-Gaussian state (\textit  {e.g.} is the fifth eigenstate of the harmonic oscillator), which here translates into negative values of the Wigner function. \relax }}{40}{figure.caption.14}\protected@file@percent }
\newlabel{fig:wigners}{{2.4}{40}{We show the Wigner functions of the vacuum state $\ket {0}$ (left) and of the Fock state $\ket {5}$ (right). While in the former case the probability distribution is a Gaussian, the latter represents a non-Gaussian state (\textit {e.g.} is the fifth eigenstate of the harmonic oscillator), which here translates into negative values of the Wigner function. \relax }{figure.caption.14}{}}
\newlabel{eq:qpd}{{2.101}{40}{Some methods in phase space}{equation.2.3.101}{}}
\newlabel{gauss_char_wigner}{{2.102}{40}{Some methods in phase space}{equation.2.3.102}{}}
\citation{Dequal2020}
\abx@aux@cite{0}{Dequal2020}
\abx@aux@segm{0}{0}{Dequal2020}
\citation{Andrews2005}
\abx@aux@cite{0}{Andrews2005}
\abx@aux@segm{0}{0}{Andrews2005}
\citation{Usenko2012a}
\abx@aux@cite{0}{Usenko2012a}
\abx@aux@segm{0}{0}{Usenko2012a}
\citation{Pirandola2021}
\abx@aux@cite{0}{Pirandola2021}
\abx@aux@segm{0}{0}{Pirandola2021}
\citation{Pirandola2021a}
\abx@aux@cite{0}{Pirandola2021a}
\abx@aux@segm{0}{0}{Pirandola2021a}
\citation{Vasylyev2011}
\abx@aux@cite{0}{Vasylyev2011}
\abx@aux@segm{0}{0}{Vasylyev2011}
\citation{Vasylyev2017}
\abx@aux@cite{0}{Vasylyev2017}
\abx@aux@segm{0}{0}{Vasylyev2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Gaussian operations and beyond}{41}{subsection.2.3.3}\protected@file@percent }
\newlabel{ssec:1_cv_channels}{{2.3.3}{41}{Gaussian operations and beyond}{subsection.2.3.3}{}}
\newlabel{eq:1_cv_gaussianCPmap}{{2.104}{41}{Gaussian operations and beyond}{equation.2.3.104}{}}
\citation{Photoncounter}
\abx@aux@cite{0}{Photoncounter}
\abx@aux@segm{0}{0}{Photoncounter}
\citation{Cirac2002Characterization}
\abx@aux@cite{0}{Cirac2002Characterization}
\abx@aux@segm{0}{0}{Cirac2002Characterization}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Measurements}{42}{subsection.2.3.4}\protected@file@percent }
\newlabel{ssec:1_cv_measurements}{{2.3.4}{42}{Measurements}{subsection.2.3.4}{}}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\citation{jacobs_2014}
\abx@aux@cite{0}{jacobs_2014}
\abx@aux@segm{0}{0}{jacobs_2014}
\citation{gardiner1985input}
\abx@aux@cite{0}{gardiner1985input}
\abx@aux@segm{0}{0}{gardiner1985input}
\citation{doherty1999feedback}
\abx@aux@cite{0}{doherty1999feedback}
\abx@aux@segm{0}{0}{doherty1999feedback}
\citation{wiseman1993interpretation}
\abx@aux@cite{0}{wiseman1993interpretation}
\abx@aux@segm{0}{0}{wiseman1993interpretation}
\citation{Jimenez2018signal}
\abx@aux@cite{0}{Jimenez2018signal}
\abx@aux@segm{0}{0}{Jimenez2018signal}
\citation{Lu2003}
\abx@aux@cite{0}{Lu2003}
\abx@aux@segm{0}{0}{Lu2003}
\citation{Aspelmayer2014cavity}
\abx@aux@cite{0}{Aspelmayer2014cavity}
\abx@aux@segm{0}{0}{Aspelmayer2014cavity}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces We depict the continuously-monitored setting under consideration. This consists on an optomechanical cavity that stores a quantum system $\rho _t$, and whose decay rate is $\gamma $, and is coupled to a bosonic bath $\rho _E$, the latter being constantly measured. This gives rise to the measurement signal, whose (stochastic) value at time $t$ is denoted by $d\mathbf  {y}_t$.\relax }}{43}{figure.caption.15}\protected@file@percent }
\newlabel{fig:cmon_scheme}{{2.5}{43}{We depict the continuously-monitored setting under consideration. This consists on an optomechanical cavity that stores a quantum system $\rho _t$, and whose decay rate is $\gamma $, and is coupled to a bosonic bath $\rho _E$, the latter being constantly measured. This gives rise to the measurement signal, whose (stochastic) value at time $t$ is denoted by $\dyt $.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Continuously-monitored systems}{43}{section.2.4}\protected@file@percent }
\newlabel{sec:1_cmon}{{2.4}{43}{Continuously-monitored systems}{section.2.4}{}}
\citation{jacobs_2014}
\abx@aux@cite{0}{jacobs_2014}
\abx@aux@segm{0}{0}{jacobs_2014}
\citation{jacobs_2014}
\abx@aux@cite{0}{jacobs_2014}
\abx@aux@segm{0}{0}{jacobs_2014}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Cavity emission and photon-detection}{44}{subsection.2.4.1}\protected@file@percent }
\newlabel{ssec:1_cmon_physics}{{2.4.1}{44}{Cavity emission and photon-detection}{subsection.2.4.1}{}}
\citation{wiseman1993interpretation}
\abx@aux@cite{0}{wiseman1993interpretation}
\abx@aux@segm{0}{0}{wiseman1993interpretation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces An optical cavity is depicted (image taken from Ref.~\blx@tocontentsinit {0}\cite {jacobs_2014}, Ch.3). Here, light is input (and output) to (and from) the cavity by the same mirror, and such two modes are splitted by the circulator.\relax }}{45}{figure.caption.16}\protected@file@percent }
\newlabel{fig:cavityCMON}{{2.6}{45}{An optical cavity is depicted (image taken from Ref.~\cite {jacobs_2014}, Ch.3). Here, light is input (and output) to (and from) the cavity by the same mirror, and such two modes are splitted by the circulator.\relax }{figure.caption.16}{}}
\newlabel{eq:commbathdelta}{{2.106}{45}{Cavity emission and photon-detection}{equation.2.4.106}{}}
\newlabel{eq:eee}{{2.107}{45}{Cavity emission and photon-detection}{equation.2.4.107}{}}
\newlabel{eq:infidB}{{2.108}{45}{Cavity emission and photon-detection}{equation.2.4.108}{}}
\newlabel{eq:}{{2.109}{45}{Cavity emission and photon-detection}{equation.2.4.109}{}}
\newlabel{eq:}{{2.111}{46}{Cavity emission and photon-detection}{equation.2.4.111}{}}
\newlabel{eq:}{{2.112}{46}{Cavity emission and photon-detection}{equation.2.4.112}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Continuous photon-detection}{46}{subsection.2.4.2}\protected@file@percent }
\newlabel{ssec:contphoto}{{2.4.2}{46}{Continuous photon-detection}{subsection.2.4.2}{}}
\newlabel{eq:photocoM}{{2.113}{47}{Continuous photon-detection}{equation.2.4.113}{}}
\newlabel{eq:cavityemission}{{2.115}{47}{Continuous photon-detection}{equation.2.4.115}{}}
\newlabel{eq:pointpro}{{2.117}{47}{Continuous photon-detection}{equation.2.4.117}{}}
\newlabel{eq:pointpro2}{{2.118}{47}{Continuous photon-detection}{equation.2.4.118}{}}
\citation{Dolinar1973}
\abx@aux@cite{0}{Dolinar1973}
\abx@aux@segm{0}{0}{Dolinar1973}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\newlabel{eq:pointpro}{{2.122}{48}{Continuous photon-detection}{equation.2.4.122}{}}
\newlabel{eq:condicompa}{{2.124}{48}{Continuous photon-detection}{equation.2.4.124}{}}
\newlabel{eq:sse}{{2.125}{48}{Continuous photon-detection}{equation.2.4.125}{}}
\newlabel{eq:foto}{{2.126}{48}{Continuous photon-detection}{equation.2.4.126}{}}
\newlabel{eq:nav1}{{2.127}{49}{Continuous photon-detection}{equation.2.4.127}{}}
\newlabel{eq:superGH}{{2.128}{49}{Continuous photon-detection}{equation.2.4.128}{}}
\citation{serafiniBOOK}
\abx@aux@cite{0}{serafiniBOOK}
\abx@aux@segm{0}{0}{serafiniBOOK}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Continuous homodyne detection}{50}{subsection.2.4.3}\protected@file@percent }
\newlabel{ssec:1_cmon_homodyne}{{2.4.3}{50}{Continuous homodyne detection}{subsection.2.4.3}{}}
\newlabel{eq:homPOM}{{2.132}{50}{Continuous homodyne detection}{equation.2.4.132}{}}
\newlabel{eq:}{{2.134}{50}{}{equation.2.4.134}{}}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\citation{jacobs_2014}
\abx@aux@cite{0}{jacobs_2014}
\abx@aux@segm{0}{0}{jacobs_2014}
\citation{JacobsStraightfoward2006}
\abx@aux@cite{0}{JacobsStraightfoward2006}
\abx@aux@segm{0}{0}{JacobsStraightfoward2006}
\citation{serafini2017quantum}
\abx@aux@cite{0}{serafini2017quantum}
\abx@aux@segm{0}{0}{serafini2017quantum}
\citation{Genoni2016conditional}
\abx@aux@cite{0}{Genoni2016conditional}
\abx@aux@segm{0}{0}{Genoni2016conditional}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces We sketch a direct homodyne detection. The beam-splitter is chosen such that the reflected part (which is subsequently measured) has only a small contribution of the local oscillator, and is dominated by the original signal associated to the system we are measuring.\relax }}{51}{figure.caption.17}\protected@file@percent }
\newlabel{fig:direhom}{{2.7}{51}{We sketch a direct homodyne detection. The beam-splitter is chosen such that the reflected part (which is subsequently measured) has only a small contribution of the local oscillator, and is dominated by the original signal associated to the system we are measuring.\relax }{figure.caption.17}{}}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\citation{wiseman1993quantum}
\abx@aux@cite{0}{wiseman1993quantum}
\abx@aux@segm{0}{0}{wiseman1993quantum}
\citation{gardiner2004handbook}
\abx@aux@cite{0}{gardiner2004handbook}
\abx@aux@segm{0}{0}{gardiner2004handbook}
\newlabel{eq:edng}{{2.139}{52}{Continuous homodyne detection}{equation.2.4.139}{}}
\newlabel{eq:variaCMON}{{2.140}{52}{Continuous homodyne detection}{equation.2.4.140}{}}
\newlabel{eq:dnito}{{2.141}{52}{Continuous homodyne detection}{equation.2.4.141}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Intermezzo II: Ito calculus}{52}{subsection.2.4.4}\protected@file@percent }
\newlabel{ssec:ito}{{2.4.4}{52}{Intermezzo II: Ito calculus}{subsection.2.4.4}{}}
\newlabel{eq:1_stoch}{{2.142}{53}{Intermezzo II: Ito calculus}{equation.2.4.142}{}}
\newlabel{eq:1_ou_integal}{{2.143}{53}{Intermezzo II: Ito calculus}{equation.2.4.143}{}}
\newlabel{eq:}{{2.144}{54}{Intermezzo II: Ito calculus}{equation.2.4.144}{}}
\newlabel{eq:}{{2.145}{54}{Intermezzo II: Ito calculus}{equation.2.4.145}{}}
\newlabel{eq:}{{2.146}{54}{Intermezzo II: Ito calculus}{equation.2.4.146}{}}
\newlabel{eq:}{{2.147}{54}{Intermezzo II: Ito calculus}{equation.2.4.147}{}}
\newlabel{eq:}{{2.148}{55}{Intermezzo II: Ito calculus}{equation.2.4.148}{}}
\newlabel{eq:}{{2.156}{56}{Intermezzo II: Ito calculus}{equation.2.4.156}{}}
\newlabel{eq:ddynexp}{{2.157}{56}{Intermezzo II: Ito calculus}{equation.2.4.157}{}}
\newlabel{eq:1introBELAVKIN}{{2.158}{56}{Intermezzo II: Ito calculus}{equation.2.4.158}{}}
\newlabel{eq:}{{2.160}{57}{Intermezzo II: Ito calculus}{equation.2.4.160}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Imperfect detection}{57}{subsection.2.4.5}\protected@file@percent }
\newlabel{eq:inefficientDETECTION}{{2.163}{57}{Imperfect detection}{equation.2.4.163}{}}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\citation{jacobs_2014}
\abx@aux@cite{0}{jacobs_2014}
\abx@aux@segm{0}{0}{jacobs_2014}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\citation{Wiseman2005optimal}
\abx@aux@cite{0}{Wiseman2005optimal}
\abx@aux@segm{0}{0}{Wiseman2005optimal}
\citation{wiseman1993interpretation}
\abx@aux@cite{0}{wiseman1993interpretation}
\abx@aux@segm{0}{0}{wiseman1993interpretation}
\citation{serafiniBOOK}
\abx@aux@cite{0}{serafiniBOOK}
\abx@aux@segm{0}{0}{serafiniBOOK}
\citation{Genoni2016conditional}
\abx@aux@cite{0}{Genoni2016conditional}
\abx@aux@segm{0}{0}{Genoni2016conditional}
\newlabel{eq:1_cmon_ineff_measu}{{2.164}{58}{Imperfect detection}{equation.2.4.164}{}}
\newlabel{eq:master_thermal}{{2.165}{58}{Imperfect detection}{equation.2.4.165}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.6}The Gaussian case}{58}{subsection.2.4.6}\protected@file@percent }
\newlabel{ssec:1_cmon_gaussian}{{2.4.6}{58}{The Gaussian case}{subsection.2.4.6}{}}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\citation{Wiseman2005optimal}
\abx@aux@cite{0}{Wiseman2005optimal}
\abx@aux@segm{0}{0}{Wiseman2005optimal}
\newlabel{eq:cmon_optimal1}{{2.166}{59}{The Gaussian case}{equation.2.4.166}{}}
\newlabel{eq:cmon_unco}{{2.168}{59}{The Gaussian case}{equation.2.4.168}{}}
\citation{JacobsStraightfoward2006}
\abx@aux@cite{0}{JacobsStraightfoward2006}
\abx@aux@segm{0}{0}{JacobsStraightfoward2006}
\citation{Wiseman2005optimal}
\abx@aux@cite{0}{Wiseman2005optimal}
\abx@aux@segm{0}{0}{Wiseman2005optimal}
\citation{Wiseman2005optimal}
\abx@aux@cite{0}{Wiseman2005optimal}
\abx@aux@segm{0}{0}{Wiseman2005optimal}
\citation{doherty1999feedback}
\abx@aux@cite{0}{doherty1999feedback}
\abx@aux@segm{0}{0}{doherty1999feedback}
\newlabel{eq:signal_dy}{{2.169}{60}{The Gaussian case}{equation.2.4.169}{}}
\newlabel{eq:cmon_LINEALSYSTEM}{{2.170}{60}{The Gaussian case}{equation.2.4.170}{}}
\citation{doherty1999feedback}
\abx@aux@cite{0}{doherty1999feedback}
\abx@aux@segm{0}{0}{doherty1999feedback}
\citation{wiseman1993quantum}
\abx@aux@cite{0}{wiseman1993quantum}
\abx@aux@segm{0}{0}{wiseman1993quantum}
\citation{Aspelmayer2014cavity}
\abx@aux@cite{0}{Aspelmayer2014cavity}
\abx@aux@segm{0}{0}{Aspelmayer2014cavity}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.7}Optomechanical systems}{61}{subsection.2.4.7}\protected@file@percent }
\newlabel{ssec:1_cmon_model}{{2.4.7}{61}{Optomechanical systems}{subsection.2.4.7}{}}
\citation{doherty1999feedback}
\abx@aux@cite{0}{doherty1999feedback}
\abx@aux@segm{0}{0}{doherty1999feedback}
\newlabel{eq:Hopto}{{2.172}{62}{Optomechanical systems}{equation.2.4.172}{}}
\newlabel{eq:}{{2.174}{62}{Optomechanical systems}{equation.2.4.174}{}}
\citation{Szorkovszky2011mechanical}
\abx@aux@cite{0}{Szorkovszky2011mechanical}
\abx@aux@segm{0}{0}{Szorkovszky2011mechanical}
\citation{doherty2012quantum}
\abx@aux@cite{0}{doherty2012quantum}
\abx@aux@segm{0}{0}{doherty2012quantum}
\newlabel{eq:cmon_LINEALSYSTEM}{{2.175}{63}{Optomechanical systems}{equation.2.4.175}{}}
\newlabel{eq:LINEAL_MECH_EQSBIS}{{2.176}{63}{Optomechanical systems}{equation.2.4.176}{}}
\newlabel{eq:}{{2.177}{63}{Optomechanical systems}{equation.2.4.177}{}}
\citation{sahakian_bad_2013}
\abx@aux@cite{0}{sahakian_bad_2013}
\abx@aux@segm{0}{0}{sahakian_bad_2013}
\newlabel{eq:LINEAL_MECH_EQSBIS}{{2.180}{64}{Optomechanical systems}{equation.2.4.180}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Statistical Inference}{64}{section.2.5}\protected@file@percent }
\newlabel{sec:1_statinf}{{2.5}{64}{Statistical Inference}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Hypothesis testing}{64}{subsection.2.5.1}\protected@file@percent }
\newlabel{ssec:1_hypo_testing}{{2.5.1}{64}{Hypothesis testing}{subsection.2.5.1}{}}
\newlabel{eq:1_statinf_symm}{{2.183}{65}{Hypothesis testing}{equation.2.5.183}{}}
\newlabel{eq:1_statinf_symm_ps}{{2.184}{65}{Hypothesis testing}{equation.2.5.184}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Single-shot quantum state discrimination}{66}{subsection.2.5.2}\protected@file@percent }
\newlabel{ssec:1_qdisc}{{2.5.2}{66}{Single-shot quantum state discrimination}{subsection.2.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Helstrom bound}{67}{subsubsection*.18}\protected@file@percent }
\newlabel{sssec:hb}{{2.5.2}{67}{The Helstrom bound}{subsubsection*.18}{}}
\newlabel{eq:1_qdisc_PSEMM}{{2.187}{67}{The Helstrom bound}{equation.2.5.187}{}}
\newlabel{eq:1_qdisc_helstom}{{2.191}{68}{The Helstrom bound}{equation.2.5.191}{}}
\@writefile{toc}{\contentsline {subsubsection}{The pure-states case}{69}{subsubsection*.19}\protected@file@percent }
\newlabel{eq:helstrom_pure}{{2.192}{69}{The pure-states case}{equation.2.5.192}{}}
\newlabel{eq:}{{2.193}{69}{The pure-states case}{equation.2.5.193}{}}
\newlabel{eq:}{{2.194}{69}{The pure-states case}{equation.2.5.194}{}}
\newlabel{eq:}{{2.196}{69}{The pure-states case}{equation.2.5.196}{}}
\newlabel{eq:projectors_pure}{{2.197}{69}{The pure-states case}{equation.2.5.197}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces We show Helstrom bound for the error probability when discriminating between two pure states as a function of their overlap $c$ (aboslute value).\relax }}{70}{figure.caption.20}\protected@file@percent }
\newlabel{fig:helpure}{{2.8}{70}{We show Helstrom bound for the error probability when discriminating between two pure states as a function of their overlap $c$ (aboslute value).\relax }{figure.caption.20}{}}
\citation{Wooters1994prettygood}
\abx@aux@cite{0}{Wooters1994prettygood}
\abx@aux@segm{0}{0}{Wooters1994prettygood}
\citation{CrokeReviewQSD}
\abx@aux@cite{0}{CrokeReviewQSD}
\abx@aux@segm{0}{0}{CrokeReviewQSD}
\citation{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Back to the classics: asymetric hypothesis testing}{71}{subsection.2.5.3}\protected@file@percent }
\newlabel{ssec:asym}{{2.5.3}{71}{Back to the classics: asymetric hypothesis testing}{subsection.2.5.3}{}}
\newlabel{eq:}{{2.203}{71}{Back to the classics: asymetric hypothesis testing}{equation.2.5.203}{}}
\@writefile{toc}{\contentsline {subsubsection}{Large deviations and assymptotic error rates}{72}{subsubsection*.21}\protected@file@percent }
\newlabel{eq:}{{2.204}{72}{Large deviations and assymptotic error rates}{equation.2.5.204}{}}
\newlabel{eq:probSEQ}{{2.205}{72}{Large deviations and assymptotic error rates}{equation.2.5.205}{}}
\newlabel{eq:}{{2.206}{73}{Large deviations and assymptotic error rates}{equation.2.5.206}{}}
\newlabel{eq:loglikiid}{{2.207}{73}{Large deviations and assymptotic error rates}{equation.2.5.207}{}}
\newlabel{eq:concenELL}{{2.209}{74}{Large deviations and assymptotic error rates}{equation.2.5.209}{}}
\newlabel{eq:chernoff}{{2.210}{74}{Large deviations and assymptotic error rates}{equation.2.5.210}{}}
\newlabel{eq:stein}{{2.211}{74}{Large deviations and assymptotic error rates}{equation.2.5.211}{}}
\@writefile{toc}{\contentsline {subsubsection}{Quantum hypothesis testing: a brief comment}{75}{subsubsection*.22}\protected@file@percent }
\newlabel{eq:optalphabeta}{{2.212}{75}{Quantum hypothesis testing: a brief comment}{equation.2.5.212}{}}
\citation{Audenaert2007Discrimination}
\abx@aux@cite{0}{Audenaert2007Discrimination}
\abx@aux@segm{0}{0}{Audenaert2007Discrimination}
\citation{Hiai1991}
\abx@aux@cite{0}{Hiai1991}
\abx@aux@segm{0}{0}{Hiai1991}
\citation{Stein2}
\abx@aux@cite{0}{Stein2}
\abx@aux@segm{0}{0}{Stein2}
\citation{Acin2005Multi}
\abx@aux@cite{0}{Acin2005Multi}
\abx@aux@segm{0}{0}{Acin2005Multi}
\citation{Vargas2021quantum}
\abx@aux@cite{0}{Vargas2021quantum}
\abx@aux@segm{0}{0}{Vargas2021quantum}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Sequetial hypothesis testing}{76}{subsection.2.5.4}\protected@file@percent }
\newlabel{ssec:sprt}{{2.5.4}{76}{Sequetial hypothesis testing}{subsection.2.5.4}{}}
\citation{Wald1948Optimum}
\abx@aux@cite{0}{Wald1948Optimum}
\abx@aux@segm{0}{0}{Wald1948Optimum}
\newlabel{eq:strongSPRT}{{2.213}{77}{Sequetial hypothesis testing}{equation.2.5.213}{}}
\newlabel{eq:decisionsASPRT}{{2.214}{77}{Sequetial hypothesis testing}{equation.2.5.214}{}}
\citation{Wald1948Optimum}
\abx@aux@cite{0}{Wald1948Optimum}
\abx@aux@segm{0}{0}{Wald1948Optimum}
\citation{probcompbook}
\abx@aux@cite{0}{probcompbook}
\abx@aux@segm{0}{0}{probcompbook}
\newlabel{eq:logsumrandom}{{2.215}{78}{Sequetial hypothesis testing}{equation.2.5.215}{}}
\newlabel{eq:stop_time}{{2.217}{78}{Sequetial hypothesis testing}{equation.2.5.217}{}}
\newlabel{eq:waldIdentity}{{2.218}{78}{Sequetial hypothesis testing}{equation.2.5.218}{}}
\newlabel{eq:waldsolved}{{2.219}{79}{Sequetial hypothesis testing}{equation.2.5.219}{}}
\newlabel{eq:waldsolved}{{2.221}{79}{Sequetial hypothesis testing}{equation.2.5.221}{}}
\citation{Wald1948Optimum}
\abx@aux@cite{0}{Wald1948Optimum}
\abx@aux@segm{0}{0}{Wald1948Optimum}
\newlabel{eq:weakErrorsSPRT}{{2.225}{80}{Sequetial hypothesis testing}{equation.2.5.225}{}}
\newlabel{eq:largeBoundaries}{{2.227}{80}{Sequetial hypothesis testing}{equation.2.5.227}{}}
\newlabel{eq:mean_ell_binarySPRT}{{2.228}{80}{Sequetial hypothesis testing}{equation.2.5.228}{}}
\newlabel{eq:sptrWaldsimple}{{2.229}{80}{Sequetial hypothesis testing}{equation.2.5.229}{}}
\citation{Gordon1976Improved}
\abx@aux@cite{0}{Gordon1976Improved}
\abx@aux@segm{0}{0}{Gordon1976Improved}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.5}Parameter estimation}{81}{subsection.2.5.5}\protected@file@percent }
\newlabel{ssec:1_stinf_estimation}{{2.5.5}{81}{Parameter estimation}{subsection.2.5.5}{}}
\newlabel{eq:}{{2.230}{81}{Parameter estimation}{equation.2.5.230}{}}
\newlabel{eq:}{{2.232}{82}{Parameter estimation}{equation.2.5.232}{}}
\citation{Meyer2021fisherinformationin}
\abx@aux@cite{0}{Meyer2021fisherinformationin}
\abx@aux@segm{0}{0}{Meyer2021fisherinformationin}
\citation{helstromBOOK}
\abx@aux@cite{0}{helstromBOOK}
\abx@aux@segm{0}{0}{helstromBOOK}
\citation{giovanetti2006quatnum}
\abx@aux@cite{0}{giovanetti2006quatnum}
\abx@aux@segm{0}{0}{giovanetti2006quatnum}
\citation{Sutton2018}
\abx@aux@cite{0}{Sutton2018}
\abx@aux@segm{0}{0}{Sutton2018}
\citation{AlgorithmsRLCsaba}
\abx@aux@cite{0}{AlgorithmsRLCsaba}
\abx@aux@segm{0}{0}{AlgorithmsRLCsaba}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Reinforcement learning}{84}{section.2.6}\protected@file@percent }
\newlabel{sec:1_rl}{{2.6}{84}{Reinforcement learning}{section.2.6}{}}
\citation{Singh1994}
\abx@aux@cite{0}{Singh1994}
\abx@aux@segm{0}{0}{Singh1994}
\citation{Mnih2013}
\abx@aux@cite{0}{Mnih2013}
\abx@aux@segm{0}{0}{Mnih2013}
\citation{Shani2013}
\abx@aux@cite{0}{Shani2013}
\abx@aux@segm{0}{0}{Shani2013}
\citation{Egorov2015}
\abx@aux@cite{0}{Egorov2015}
\abx@aux@segm{0}{0}{Egorov2015}
\citation{Zhu2018}
\abx@aux@cite{0}{Zhu2018}
\abx@aux@segm{0}{0}{Zhu2018}
\newlabel{fig:rldep}{{\caption@xref {fig:rldep}{ on input line 12}}{85}{Reinforcement learning}{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces We depict the interaction with agent and environment in fully (left) and partially observable (right) cases.\relax }}{85}{figure.caption.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Bandit problems}{86}{subsection.2.6.1}\protected@file@percent }
\newlabel{ssec:1_rl_bandit}{{2.6.1}{86}{Bandit problems}{subsection.2.6.1}{}}
\citation{Lai1985}
\abx@aux@cite{0}{Lai1985}
\abx@aux@segm{0}{0}{Lai1985}
\citation{Thompson1933}
\abx@aux@cite{0}{Thompson1933}
\abx@aux@segm{0}{0}{Thompson1933}
\citation{banditbook}
\abx@aux@cite{0}{banditbook}
\abx@aux@segm{0}{0}{banditbook}
\citation{regRL1}
\abx@aux@cite{0}{regRL1}
\abx@aux@segm{0}{0}{regRL1}
\citation{regRL2}
\abx@aux@cite{0}{regRL2}
\abx@aux@segm{0}{0}{regRL2}
\citation{thesisRegret}
\abx@aux@cite{0}{thesisRegret}
\abx@aux@segm{0}{0}{thesisRegret}
\citation{QlearningUCB}
\abx@aux@cite{0}{QlearningUCB}
\abx@aux@segm{0}{0}{QlearningUCB}
\newlabel{eq:CumRegegret}{{2.234}{87}{Bandit problems}{equation.2.6.234}{}}
\newlabel{eq:RLBOUND}{{2.235}{87}{Bandit problems}{equation.2.6.235}{}}
\citation{Lai1985}
\abx@aux@cite{0}{Lai1985}
\abx@aux@segm{0}{0}{Lai1985}
\citation{Agrawal1995}
\abx@aux@cite{0}{Agrawal1995}
\abx@aux@segm{0}{0}{Agrawal1995}
\citation{Auer2002}
\abx@aux@cite{0}{Auer2002}
\abx@aux@segm{0}{0}{Auer2002}
\citation{Thompson1933}
\abx@aux@cite{0}{Thompson1933}
\abx@aux@segm{0}{0}{Thompson1933}
\citation{Thompson1935}
\abx@aux@cite{0}{Thompson1935}
\abx@aux@segm{0}{0}{Thompson1935}
\citation{Scott2010}
\abx@aux@cite{0}{Scott2010}
\abx@aux@segm{0}{0}{Scott2010}
\citation{Russo2018}
\abx@aux@cite{0}{Russo2018}
\abx@aux@segm{0}{0}{Russo2018}
\citation{TSoptimal}
\abx@aux@cite{0}{TSoptimal}
\abx@aux@segm{0}{0}{TSoptimal}
\citation{Auer2002}
\abx@aux@cite{0}{Auer2002}
\abx@aux@segm{0}{0}{Auer2002}
\citation{banditbook}
\abx@aux@cite{0}{banditbook}
\abx@aux@segm{0}{0}{banditbook}
\newlabel{alg:epgreedybandit}{{1}{89}{Bandit problems}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces We detail the $\epsilon $-greedy policy for bandit problems.\relax }}{89}{algocf.1}\protected@file@percent }
\newlabel{eq:Qband}{{2.236}{89}{Bandit problems}{equation.2.6.236}{}}
\newlabel{eq:ucbeq}{{2.237}{89}{Bandit problems}{equation.2.6.237}{}}
\newlabel{eq:ucbeq2}{{2.238}{89}{Bandit problems}{equation.2.6.238}{}}
\newlabel{alg:ucbandit}{{\caption@xref {alg:ucbandit}{ on input line 66}}{90}{Bandit problems}{algocf.caption.24}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces UCB for bandit problems.\relax }}{90}{algocf.2}\protected@file@percent }
\citation{Thompson1935}
\abx@aux@cite{0}{Thompson1935}
\abx@aux@segm{0}{0}{Thompson1935}
\citation{regTS1}
\abx@aux@cite{0}{regTS1}
\abx@aux@segm{0}{0}{regTS1}
\citation{workTSFOLK}
\abx@aux@cite{0}{workTSFOLK}
\abx@aux@segm{0}{0}{workTSFOLK}
\citation{Russo2018}
\abx@aux@cite{0}{Russo2018}
\abx@aux@segm{0}{0}{Russo2018}
\newlabel{eq:betaDistro}{{2.239}{91}{Bandit problems}{equation.2.6.239}{}}
\newlabel{alg:tsber}{{\caption@xref {alg:tsber}{ on input line 93}}{91}{Bandit problems}{algocf.caption.25}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces TS for Bernoulli bandit problems.\relax }}{91}{algocf.3}\protected@file@percent }
\citation{simpleRegretMunoz}
\abx@aux@cite{0}{simpleRegretMunoz}
\abx@aux@segm{0}{0}{simpleRegretMunoz}
\citation{Bellman2003}
\abx@aux@cite{0}{Bellman2003}
\abx@aux@segm{0}{0}{Bellman2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Value functions and the Bellman equation}{92}{subsection.2.6.2}\protected@file@percent }
\newlabel{ssec:1_rl_seqDec}{{2.6.2}{92}{Value functions and the Bellman equation}{subsection.2.6.2}{}}
\newlabel{eq:returnGT}{{2.241}{92}{Value functions and the Bellman equation}{equation.2.6.241}{}}
\newlabel{eq:vFunc}{{2.242}{92}{Value functions and the Bellman equation}{equation.2.6.242}{}}
\newlabel{eq:vBell}{{2.243}{93}{Value functions and the Bellman equation}{equation.2.6.243}{}}
\newlabel{eq:vBellOp}{{2.244}{93}{Value functions and the Bellman equation}{equation.2.6.244}{}}
\newlabel{eq:bellqas}{{2.247}{94}{Value functions and the Bellman equation}{equation.2.6.247}{}}
\newlabel{eq:qBellOp}{{2.247}{94}{Value functions and the Bellman equation}{equation.2.6.247}{}}
\citation{Watkins1989}
\abx@aux@cite{0}{Watkins1989}
\abx@aux@segm{0}{0}{Watkins1989}
\citation{Mnih2013}
\abx@aux@cite{0}{Mnih2013}
\abx@aux@segm{0}{0}{Mnih2013}
\citation{ddpg}
\abx@aux@cite{0}{ddpg}
\abx@aux@segm{0}{0}{ddpg}
\citation{algsrl}
\abx@aux@cite{0}{algsrl}
\abx@aux@segm{0}{0}{algsrl}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Q-learning}{95}{subsection.2.6.3}\protected@file@percent }
\newlabel{ssec:1_rl_qlearning}{{2.6.3}{95}{Q-learning}{subsection.2.6.3}{}}
\newlabel{eq:QLUPDATERULE}{{2.248}{95}{Q-learning}{equation.2.6.248}{}}
\citation{Watkins1989}
\abx@aux@cite{0}{Watkins1989}
\abx@aux@segm{0}{0}{Watkins1989}
\citation{Sutton2018}
\abx@aux@cite{0}{Sutton2018}
\abx@aux@segm{0}{0}{Sutton2018}
\citation{Sutton2018}
\abx@aux@cite{0}{Sutton2018}
\abx@aux@segm{0}{0}{Sutton2018}
\newlabel{alg:ql}{{\caption@xref {alg:ql}{ on input line 17}}{96}{Q-learning}{algocf.caption.26}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Q-learning pseudo-code.\relax }}{96}{algocf.4}\protected@file@percent }
\newlabel{eq:qLConv}{{2.250}{96}{Q-learning}{equation.2.6.250}{}}
\citation{Sutton2018}
\abx@aux@cite{0}{Sutton2018}
\abx@aux@segm{0}{0}{Sutton2018}
\citation{Sutton2018}
\abx@aux@cite{0}{Sutton2018}
\abx@aux@segm{0}{0}{Sutton2018}
\citation{Bellman2003}
\abx@aux@cite{0}{Bellman2003}
\abx@aux@segm{0}{0}{Bellman2003}
\citation{Sutton2018}
\abx@aux@cite{0}{Sutton2018}
\abx@aux@segm{0}{0}{Sutton2018}
\citation{AlgorithmsRLCsaba}
\abx@aux@cite{0}{AlgorithmsRLCsaba}
\abx@aux@segm{0}{0}{AlgorithmsRLCsaba}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Model aware methods}{97}{subsection.2.6.4}\protected@file@percent }
\newlabel{ssec:1_rl_dp}{{2.6.4}{97}{Model aware methods}{subsection.2.6.4}{}}
\citation{Bellman2003}
\abx@aux@cite{0}{Bellman2003}
\abx@aux@segm{0}{0}{Bellman2003}
\newlabel{eq:dpL}{{2.252}{98}{Model aware methods}{equation.2.6.252}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Discussion}{99}{subsection.2.6.5}\protected@file@percent }
\newlabel{ssec:1_rl_discu}{{2.6.5}{99}{Discussion}{subsection.2.6.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning in the darkness}{101}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:RLCOH}{{3}{101}{Learning in the darkness}{chapter.3}{}}
\citation{Glauber1963Coherent}
\abx@aux@cite{0}{Glauber1963Coherent}
\abx@aux@segm{0}{0}{Glauber1963Coherent}
\citation{Osaki1996Derivation}
\abx@aux@cite{0}{Osaki1996Derivation}
\abx@aux@segm{0}{0}{Osaki1996Derivation}
\citation{catstates0}
\abx@aux@cite{0}{catstates0}
\abx@aux@segm{0}{0}{catstates0}
\citation{catsates1_RL}
\abx@aux@cite{0}{catsates1_RL}
\abx@aux@segm{0}{0}{catsates1_RL}
\citation{Dolinar}
\abx@aux@cite{0}{Dolinar}
\abx@aux@segm{0}{0}{Dolinar}
\citation{Takeoka2005}
\abx@aux@cite{0}{Takeoka2005}
\abx@aux@segm{0}{0}{Takeoka2005}
\citation{Cook2007}
\abx@aux@cite{0}{Cook2007}
\abx@aux@segm{0}{0}{Cook2007}
\citation{Geremia2004}
\abx@aux@cite{0}{Geremia2004}
\abx@aux@segm{0}{0}{Geremia2004}
\citation{DaSilva2013}
\abx@aux@cite{0}{DaSilva2013}
\abx@aux@segm{0}{0}{DaSilva2013}
\citation{DiMario2018}
\abx@aux@cite{0}{DiMario2018}
\abx@aux@segm{0}{0}{DiMario2018}
\newlabel{eq:hel_coh}{{3.1}{102}{Learning in the darkness}{equation.3.0.1}{}}
\citation{Dequal2020}
\abx@aux@cite{0}{Dequal2020}
\abx@aux@segm{0}{0}{Dequal2020}
\citation{Andrews2005}
\abx@aux@cite{0}{Andrews2005}
\abx@aux@segm{0}{0}{Andrews2005}
\citation{Usenko2012a}
\abx@aux@cite{0}{Usenko2012a}
\abx@aux@segm{0}{0}{Usenko2012a}
\citation{Pirandola2021}
\abx@aux@cite{0}{Pirandola2021}
\abx@aux@segm{0}{0}{Pirandola2021}
\citation{Pirandola2021a}
\abx@aux@cite{0}{Pirandola2021a}
\abx@aux@segm{0}{0}{Pirandola2021a}
\citation{Vasylyev2011}
\abx@aux@cite{0}{Vasylyev2011}
\abx@aux@segm{0}{0}{Vasylyev2011}
\citation{Vasylyev2017}
\abx@aux@cite{0}{Vasylyev2017}
\abx@aux@segm{0}{0}{Vasylyev2017}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Coherent-state quantum receivers}{104}{section.3.1}\protected@file@percent }
\newlabel{sec:rl_coh_quantum_receivers}{{3.1}{104}{Coherent-state quantum receivers}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces We show the Wigner function for the two candidate states $\ket {\alpha _k}, \tmspace  +\thickmuskip {.2777em} k=0,1$, which are Gaussian distribution functions. We restrict to real amplitudes, since in the binary case one can always rotate the frame (assuming we know the direction to do so).\relax }}{105}{figure.caption.27}\protected@file@percent }
\newlabel{fig:300coh}{{3.1}{105}{We show the Wigner function for the two candidate states $\ket {\alpha _k}, \; k=0,1$, which are Gaussian distribution functions. We restrict to real amplitudes, since in the binary case one can always rotate the frame (assuming we know the direction to do so).\relax }{figure.caption.27}{}}
\citation{Limit2021Roberson}
\abx@aux@cite{0}{Limit2021Roberson}
\abx@aux@segm{0}{0}{Limit2021Roberson}
\citation{opGaussDet}
\abx@aux@cite{0}{opGaussDet}
\abx@aux@segm{0}{0}{opGaussDet}
\citation{Winter2021Bosonic}
\abx@aux@cite{0}{Winter2021Bosonic}
\abx@aux@segm{0}{0}{Winter2021Bosonic}
\citation{serafiniBOOK}
\abx@aux@cite{0}{serafiniBOOK}
\abx@aux@segm{0}{0}{serafiniBOOK}
\citation{serafiniBOOK}
\abx@aux@cite{0}{serafiniBOOK}
\abx@aux@segm{0}{0}{serafiniBOOK}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Gaussian receivers}{106}{subsection.3.1.1}\protected@file@percent }
\citation{nongaussian1}
\abx@aux@cite{0}{nongaussian1}
\abx@aux@segm{0}{0}{nongaussian1}
\citation{Kennedy1973a}
\abx@aux@cite{0}{Kennedy1973a}
\abx@aux@segm{0}{0}{Kennedy1973a}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An implementation of homodyne measurement is shown, which consists on mixing the incoming signal with a coherent state of strong intensity, known as Local Oscillator, by a balanced beam-splitter. The difference in signals' intensities between the two exiting ports constitutes the homodyne measurement outcome~\blx@tocontentsinit {0}\cite {serafiniBOOK}.\relax }}{107}{figure.caption.28}\protected@file@percent }
\newlabel{fig:homodynereceiver}{{3.2}{107}{An implementation of homodyne measurement is shown, which consists on mixing the incoming signal with a coherent state of strong intensity, known as Local Oscillator, by a balanced beam-splitter. The difference in signals' intensities between the two exiting ports constitutes the homodyne measurement outcome~\cite {serafiniBOOK}.\relax }{figure.caption.28}{}}
\newlabel{eq:gaussian_receiver_homo}{{3.6}{107}{Gaussian receivers}{equation.3.1.6}{}}
\newlabel{ssec:1_gaussian_receivers}{{3.1.1}{107}{Gaussian receivers}{equation.3.1.6}{}}
\newlabel{fig:312shiftsA}{{3.3a}{108}{\relax }{figure.caption.29}{}}
\newlabel{sub@fig:312shiftsA}{{a}{108}{\relax }{figure.caption.29}{}}
\newlabel{fig:312shiftsB}{{3.3b}{108}{\relax }{figure.caption.29}{}}
\newlabel{sub@fig:312shiftsB}{{b}{108}{\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces We show the Wigner functions for (panel $a$) the coherent states $\ket {\pm \alpha }$ and (panel $b$) the displaced coherent states $\ket {0}$ and $\ket {2 \alpha }$\relax }}{108}{figure.caption.29}\protected@file@percent }
\newlabel{fig:312shifts}{{3.3}{108}{We show the Wigner functions for (panel $a$) the coherent states $\ket {\pm \alpha }$ and (panel $b$) the displaced coherent states $\ket {0}$ and $\ket {2 \alpha }$\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Kennedy receivers}{108}{subsection.3.1.2}\protected@file@percent }
\newlabel{ssec:rlcoh_kennedyreceiver}{{3.1.2}{108}{Kennedy receivers}{subsection.3.1.2}{}}
\newlabel{eq:ps_ken}{{3.8}{109}{Kennedy receivers}{equation.3.1.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Optimized Kennedy receiver}{109}{subsubsection*.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces We show the Kennedy receiver, which in the case of coherent states' intensity being $\alpha $, it reduces to $\beta = -\alpha $.\relax }}{109}{figure.caption.30}\protected@file@percent }
\newlabel{fig:kennedy_receiver}{{3.4}{109}{We show the Kennedy receiver, which in the case of coherent states' intensity being $\alpha $, it reduces to $\beta = -\alpha $.\relax }{figure.caption.30}{}}
\citation{Bell1964}
\abx@aux@cite{0}{Bell1964}
\abx@aux@segm{0}{0}{Bell1964}
\citation{Bert05}
\abx@aux@cite{0}{Bert05}
\abx@aux@segm{0}{0}{Bert05}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces We show the optimization landscape for the $\beta $-Kennedy receiver, at a fixed intensity $\alpha =0.2$. which in the case of coherent states' intensity being $\alpha $, it reduces to $\beta = -\alpha $.\relax }}{110}{figure.caption.32}\protected@file@percent }
\newlabel{fig:optiland}{{3.5}{110}{We show the optimization landscape for the $\beta $-Kennedy receiver, at a fixed intensity $\alpha =0.2$. which in the case of coherent states' intensity being $\alpha $, it reduces to $\beta = -\alpha $.\relax }{figure.caption.32}{}}
\citation{Dolinar1973}
\abx@aux@cite{0}{Dolinar1973}
\abx@aux@segm{0}{0}{Dolinar1973}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces We compare the success probabilities for homodyne receiver, Kennedy receiver, and optimized-Kennedy receiver with the Helstrom bound. Data in both panels is the same, however in right one we show the different with Helstrom bound, in order to help visualization.\relax }}{111}{figure.caption.33}\protected@file@percent }
\newlabel{fig:kenn_compa1}{{3.6}{111}{We compare the success probabilities for homodyne receiver, Kennedy receiver, and optimized-Kennedy receiver with the Helstrom bound. Data in both panels is the same, however in right one we show the different with Helstrom bound, in order to help visualization.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Dolinar receivers}{111}{subsection.3.1.3}\protected@file@percent }
\newlabel{ssec:rlcoh_dolinarreceiver}{{3.1.3}{111}{Dolinar receivers}{subsection.3.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces We show a Dolinar-like receiver. For an infinite sequence measurements and feedforward operations, such a scheme attains the Helstrom bound for binary coherent-state discrimination.\relax }}{112}{figure.caption.34}\protected@file@percent }
\newlabel{fig:313dolinar_setup}{{3.7}{112}{We show a Dolinar-like receiver. For an infinite sequence measurements and feedforward operations, such a scheme attains the Helstrom bound for binary coherent-state discrimination.\relax }{figure.caption.34}{}}
\newlabel{ops:end}{{3}{113}{Dolinar receivers}{Item.10}{}}
\newlabel{eq:313singLayProb}{{3.10}{113}{Dolinar receivers}{equation.3.1.10}{}}
\newlabel{eq:}{{3.11}{113}{Dolinar receivers}{equation.3.1.11}{}}
\newlabel{eq:psdol}{{3.12}{113}{Dolinar receivers}{equation.3.1.12}{}}
\citation{revisiting2011Assalini}
\abx@aux@cite{0}{revisiting2011Assalini}
\abx@aux@segm{0}{0}{revisiting2011Assalini}
\citation{revisiting2011Assalini}
\abx@aux@cite{0}{revisiting2011Assalini}
\abx@aux@segm{0}{0}{revisiting2011Assalini}
\citation{Dolinar}
\abx@aux@cite{0}{Dolinar}
\abx@aux@segm{0}{0}{Dolinar}
\citation{quasicontinuous}
\abx@aux@cite{0}{quasicontinuous}
\abx@aux@segm{0}{0}{quasicontinuous}
\citation{Dolinar}
\abx@aux@cite{0}{Dolinar}
\abx@aux@segm{0}{0}{Dolinar}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces We show the original proposal of the Dolinar receiver, which works in continuous-time. This image is taken from Ref.~\blx@tocontentsinit {0}\cite {revisiting2011Assalini}. Instead of splitting the coherent-state in space by a beam-splitter array, we here have the signal spread in time, as explained in the main-body. The $\ell $-dependence in the conditional displacements is now translated into a time-dependence for $u(t)$, which in turn depends on the parity of the measurement outcomes as captured by the function $z(t)$. After the pulse-duration $T$, a decision for the phase of the coherent state is done via the function $z(T)$.\relax }}{114}{figure.caption.35}\protected@file@percent }
\newlabel{fig:dolinar_cont}{{3.8}{114}{We show the original proposal of the Dolinar receiver, which works in continuous-time. This image is taken from Ref.~\cite {revisiting2011Assalini}. Instead of splitting the coherent-state in space by a beam-splitter array, we here have the signal spread in time, as explained in the main-body. The $\ell $-dependence in the conditional displacements is now translated into a time-dependence for $u(t)$, which in turn depends on the parity of the measurement outcomes as captured by the function $z(t)$. After the pulse-duration $T$, a decision for the phase of the coherent state is done via the function $z(T)$.\relax }{figure.caption.35}{}}
\newlabel{eq:32OptSuc}{{3.13}{114}{Dolinar receivers}{equation.3.1.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Time-domain Dolinar receiver $\&$ optimality}{114}{subsection.3.1.4}\protected@file@percent }
\newlabel{ssec:tdol}{{3.1.4}{114}{Time-domain Dolinar receiver $\&$ optimality}{subsection.3.1.4}{}}
\citation{Acin2005Multi}
\abx@aux@cite{0}{Acin2005Multi}
\abx@aux@segm{0}{0}{Acin2005Multi}
\citation{revisiting2011Assalini}
\abx@aux@cite{0}{revisiting2011Assalini}
\abx@aux@segm{0}{0}{revisiting2011Assalini}
\citation{Takeoka2005}
\abx@aux@cite{0}{Takeoka2005}
\abx@aux@segm{0}{0}{Takeoka2005}
\citation{sychLeuchs}
\abx@aux@cite{0}{sychLeuchs}
\abx@aux@segm{0}{0}{sychLeuchs}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}The sequential structure of Dolinar receiver}{116}{subsection.3.1.5}\protected@file@percent }
\newlabel{ssec:3bis_intuition}{{3.1.5}{116}{The sequential structure of Dolinar receiver}{subsection.3.1.5}{}}
\newlabel{eq:0dol}{{3.14}{116}{The sequential structure of Dolinar receiver}{equation.3.1.14}{}}
\newlabel{eq:1dol}{{3.15}{116}{The sequential structure of Dolinar receiver}{equation.3.1.15}{}}
\newlabel{eq:1dol_eta}{{3.16}{117}{The sequential structure of Dolinar receiver}{equation.3.1.16}{}}
\newlabel{eq:rlcoh_bayes_prior}{{3.17}{117}{The sequential structure of Dolinar receiver}{equation.3.1.17}{}}
\newlabel{eq:1dol_post}{{3.18}{117}{The sequential structure of Dolinar receiver}{equation.3.1.18}{}}
\newlabel{eq:2dol}{{3.20}{117}{The sequential structure of Dolinar receiver}{equation.3.1.20}{}}
\newlabel{eq:depend}{{3.25}{118}{The sequential structure of Dolinar receiver}{equation.3.1.25}{}}
\newlabel{eq:Ldol}{{3.27}{118}{The sequential structure of Dolinar receiver}{equation.3.1.27}{}}
\newlabel{eq:symmetry_kenn}{{3.32}{119}{The sequential structure of Dolinar receiver}{equation.3.1.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The model-aware approach}{119}{section.3.2}\protected@file@percent }
\newlabel{sec:rlcoh_model_aware_approach_intro}{{3.2}{119}{The model-aware approach}{section.3.2}{}}
\newlabel{fig:dpre1}{{3.9a}{120}{\relax }{figure.caption.36}{}}
\newlabel{sub@fig:dpre1}{{a}{120}{\relax }{figure.caption.36}{}}
\newlabel{fig:dpre2}{{3.9b}{120}{\relax }{figure.caption.36}{}}
\newlabel{sub@fig:dpre2}{{b}{120}{\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces We show the difference between the best probability of success attainable for a fix $L$ and the optimal probability of success in discriminating BPSK coherent states. The results were obtained by dynamic programming. \textit  {Left panel}: the attenuations at the beam-splitters are fixed in such a way that each partial measurement deals with a state having the same intensity $\alpha ^{\ell }_k=\frac  {\alpha _k}{\sqrt  {L-1}}$ for all $\ell $. \textit  {Right panel}: we compare the advantage of adapting beam-splitter transmisivities (dashed lines) as compared to the receivers considered in left panel (solid lines).\relax }}{120}{figure.caption.36}\protected@file@percent }
\newlabel{fig:dp_resu}{{3.9}{120}{We show the difference between the best probability of success attainable for a fix $L$ and the optimal probability of success in discriminating BPSK coherent states. The results were obtained by dynamic programming. \textit {Left panel}: the attenuations at the beam-splitters are fixed in such a way that each partial measurement deals with a state having the same intensity $\alpha ^{\ell }_k=\frac {\alpha _k}{\sqrt {L-1}}$ for all $\ell $. \textit {Right panel}: we compare the advantage of adapting beam-splitter transmisivities (dashed lines) as compared to the receivers considered in left panel (solid lines).\relax }{figure.caption.36}{}}
\newlabel{eq:belief}{{3.33}{120}{The model-aware approach}{equation.3.2.33}{}}
\newlabel{eq:belUp}{{3.34}{120}{The model-aware approach}{equation.3.2.34}{}}
\citation{rbf_interpolate}
\abx@aux@cite{0}{rbf_interpolate}
\abx@aux@segm{0}{0}{rbf_interpolate}
\citation{scipy}
\abx@aux@cite{0}{scipy}
\abx@aux@segm{0}{0}{scipy}
\newlabel{eq:opBellLQSD}{{3.35}{121}{The model-aware approach}{equation.3.2.35}{}}
\newlabel{eq:opBellEllQSD}{{3.36}{121}{The model-aware approach}{equation.3.2.36}{}}
\citation{Dolinar}
\abx@aux@cite{0}{Dolinar}
\abx@aux@segm{0}{0}{Dolinar}
\citation{Takeoka2005}
\abx@aux@cite{0}{Takeoka2005}
\abx@aux@segm{0}{0}{Takeoka2005}
\citation{revisiting2011Assalini}
\abx@aux@cite{0}{revisiting2011Assalini}
\abx@aux@segm{0}{0}{revisiting2011Assalini}
\@writefile{toc}{\contentsline {subsubsection}{Unveiling Dolinar's strategy from the numerics}{122}{subsubsection*.37}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces \small  {We show the structure of the optimal policy for a $10$-Dolinar receiver, where we consider $\alpha =0.4$. Each panel corresponds to a different string of observations obtained through a possible experiment realisation. At each layer, the optimal policy obtained through dynamic programming instructs the agent to perform an action (a displacement) according to the current belief $b_\ell $ on state $\ket {\alpha }$; such belief is updated as more information about the environment state is acquired. Thus, in order to obtain this plot, we propagate the initial prior (set to $\frac  {1}{2}$) as per Eq.~\ref  {eq:belUp}, using the corresponding measurement outcome. Then, we find the interpolate the optimal action that corresponding to that belief, and repeat this along the $10$ layers. Here, we do also compute the total probability of finding each sequence, as per $p(o_{1:L}) = \DOTSB \sum@ \slimits@ _k p(o_{1:L}|\alpha _k)\eta _k$. Such probability is extremely low for the rightmost sequence (all ones), here rounded to zero, and considerably high for the sequence of all zero outcomes (see discussion in the main body).}\relax }}{123}{figure.caption.38}\protected@file@percent }
\newlabel{fig:strategy_dp}{{3.10}{123}{\small {We show the structure of the optimal policy for a $10$-Dolinar receiver, where we consider $\alpha =0.4$. Each panel corresponds to a different string of observations obtained through a possible experiment realisation. At each layer, the optimal policy obtained through dynamic programming instructs the agent to perform an action (a displacement) according to the current belief $b_\ell $ on state $\ket {\alpha }$; such belief is updated as more information about the environment state is acquired. Thus, in order to obtain this plot, we propagate the initial prior (set to $\frac {1}{2}$) as per Eq.~\ref {eq:belUp}, using the corresponding measurement outcome. Then, we find the interpolate the optimal action that corresponding to that belief, and repeat this along the $10$ layers. Here, we do also compute the total probability of finding each sequence, as per $p(o_{1:L}) = \sum _k p(o_{1:L}|\alpha _k)\eta _k$. Such probability is extremely low for the rightmost sequence (all ones), here rounded to zero, and considerably high for the sequence of all zero outcomes (see discussion in the main body).}\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The model-free approach}{124}{section.3.3}\protected@file@percent }
\newlabel{sec:rl_coh_model_free}{{3.3}{124}{The model-free approach}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Reinforcement learning the Dolinar receiver}{124}{subsection.3.3.1}\protected@file@percent }
\newlabel{ssec:rl_cohql}{{3.3.1}{124}{Reinforcement learning the Dolinar receiver}{subsection.3.3.1}{}}
\newlabel{eq:sTr}{{3.37}{125}{Reinforcement learning the Dolinar receiver}{equation.3.3.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Q-learning the Dolinar receiver}{126}{subsection.3.3.2}\protected@file@percent }
\newlabel{ssec:rlcoh_qlearning}{{3.3.2}{126}{Q-learning the Dolinar receiver}{subsection.3.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces We benchmark traditional Q-learning with different \textit  {schedules} on $\epsilon $ as the episode number increases. The figures of merit are averaged over $A=48$ agents and show the corresponding uncertainty region. \relax }}{129}{figure.caption.39}\protected@file@percent }
\newlabel{fig:compEpsql}{{3.11}{129}{We benchmark traditional Q-learning with different \textit {schedules} on $\epsilon $ as the episode number increases. The figures of merit are averaged over $A=48$ agents and show the corresponding uncertainty region. \relax }{figure.caption.39}{}}
\newlabel{eq:315diff}{{3.42}{130}{Q-learning the Dolinar receiver}{equation.3.3.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Optimal state-action values $\&$ convergence}{130}{subsection.3.3.3}\protected@file@percent }
\newlabel{ssec:optVal}{{3.3.3}{130}{Optimal state-action values $\&$ convergence}{subsection.3.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Density plot of the difference between the estimated $Q$-values for guessing ``plus'' and ``minus'' as a function of the displacements at the first and second layer, for each possible sequence of outcomes, with $\alpha =0.4$. The shaded areas correspond to the regions where the optimal guess, taken according to maximum-likelihood, is ``plus''. The white dots corresponds to the optimal values of the displacements for the proper discretization). \relax }}{131}{figure.caption.40}\protected@file@percent }
\newlabel{fig:315guess}{{3.12}{131}{Density plot of the difference between the estimated $Q$-values for guessing ``plus'' and ``minus'' as a function of the displacements at the first and second layer, for each possible sequence of outcomes, with $\alpha =0.4$. The shaded areas correspond to the regions where the optimal guess, taken according to maximum-likelihood, is ``plus''. The white dots corresponds to the optimal values of the displacements for the proper discretization). \relax }{figure.caption.40}{}}
\newlabel{eq:OptQLL}{{3.47}{133}{Optimal state-action values $\&$ convergence}{equation.3.3.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces We plot different values of the Q-estimates, after $10^{8}$ episodes of random exploration ($\epsilon = 1$), updating the Q-estimates according to Q-learning (see Algorithm 1). The random exploration is used in order to ensure that, at \textit  {finite} number of episodes, all state-action pairs were equally visited on average. The lower plot corresponds to the estimates $\hat  {Q}(a_0)$, and it is compared with the optimal success probability as a function of $a_0$, i.e. $P_*^{(L=2)}(\alpha , a_0) = \DOTSB \sum@ \slimits@ _{o1}\tmspace  +\thickmuskip {.2777em}p(o_1; a_0) \tmspace  +\thickmuskip {.2777em} \begingroup \setbox \z@ \hbox {\thinmuskip 0mu \medmuskip \m@ne mu\thickmuskip \@ne mu \setbox \tw@ \hbox {$\text  {max}\mathsurround \z@ $}\kern -\wd \tw@ ${}\text  {max}{}\mathsurround \z@ $}\edef {\mathop {\kern \z@ \text  {max}}\limits _{a_1}} \DOTSB \sum@ \slimits@ _{o_2} p(o_2|h_1, a_1) \tmspace  +\thickmuskip {.2777em} \begingroup \setbox \z@ \hbox {\thinmuskip 0mu \medmuskip \m@ne mu\thickmuskip \@ne mu \setbox \tw@ \hbox {$\text  {max }\mathsurround \z@ $}\kern -\wd \tw@ ${}\text  {max }{}\mathsurround \z@ $}\edef {\mathop {\kern \z@ \text  {max }}\limits _{a_2 = \pm }} p(\pm \alpha | h_2) pr(\pm \alpha )$.\relax }}{134}{figure.caption.41}\protected@file@percent }
\newlabel{fig:profiles}{{3.13}{134}{We plot different values of the Q-estimates, after $10^{8}$ episodes of random exploration ($\epsilon = 1$), updating the Q-estimates according to Q-learning (see Algorithm 1). The random exploration is used in order to ensure that, at \textit {finite} number of episodes, all state-action pairs were equally visited on average. The lower plot corresponds to the estimates $\hat {Q}(a_0)$, and it is compared with the optimal success probability as a function of $a_0$, i.e. $P_*^{(L=2)}(\alpha , a_0) = \sum _{o1}\;p(o_1; a_0) \; \underset {a_1}{\text {max}} \sum _{o_2} p(o_2|h_1, a_1) \; \underset {a_2 = \pm }{\text {max }} p(\pm \alpha | h_2) pr(\pm \alpha )$.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}A little help from my bandit friends}{135}{subsection.3.3.4}\protected@file@percent }
\newlabel{ssec:rlcoh_dolinar_plus_bandit}{{3.3.4}{135}{A little help from my bandit friends}{subsection.3.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces We show the evolution of the cumulative regret for three different policies: $\varepsilon $-greedy, UCB and TS in a bandit setting. The mean values considered are associated to success probabilities of Kennedy-like receivers, as introduced in Sec.~\ref  {ssec:rlcoh_kennedyreceiver}. All curves are averaged over $10^{3}$ agents. Specifically, they are Bernoulli distributions with different mean values, each associated to different configurations of a quantum receiver parametrized by a value $\beta $. For \textit  {bandit problem 1}, we considered $\beta \in \{0, -\alpha , \beta ^{*}\}$, with $\alpha = 0.4$ and $\beta ^{*} = -0.74$. Furthermore, we compare the asymptotic behaviour of TS, studying \textit  {bandit problem 2}, where $\beta \in \{-\alpha , \beta ^{*}, -1.5 \alpha \}$, shown in the inset plot.\relax }}{136}{figure.caption.42}\protected@file@percent }
\newlabel{fig:bandfig}{{3.14}{136}{We show the evolution of the cumulative regret for three different policies: $\varepsilon $-greedy, UCB and TS in a bandit setting. The mean values considered are associated to success probabilities of Kennedy-like receivers, as introduced in Sec.~\ref {ssec:rlcoh_kennedyreceiver}. All curves are averaged over $10^{3}$ agents. Specifically, they are Bernoulli distributions with different mean values, each associated to different configurations of a quantum receiver parametrized by a value $\beta $. For \textit {bandit problem 1}, we considered $\beta \in \{0, -\alpha , \beta ^{*}\}$, with $\alpha = 0.4$ and $\beta ^{*} = -0.74$. Furthermore, we compare the asymptotic behaviour of TS, studying \textit {bandit problem 2}, where $\beta \in \{-\alpha , \beta ^{*}, -1.5 \alpha \}$, shown in the inset plot.\relax }{figure.caption.42}{}}
\citation{Russo2018}
\abx@aux@cite{0}{Russo2018}
\abx@aux@segm{0}{0}{Russo2018}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces We show the learning curves for the enhanced Q-learning agents via bandit methods. On the upper plot we despict $\mathbf  {R}_{t}$, the agent's success rate per episode, whereas on the bottom plot we despict $\mathbf  {P}_{t}$, the success probability of the agent's recommended actions at episode $t$, $\lbrace a_{\ell }^{(t)*}\rbrace $. Each of the learning curves is averaged over 48 agents; the amplitude was fixed to $\alpha = 0.4$.\relax }}{138}{figure.caption.43}\protected@file@percent }
\newlabel{fig:threemethods}{{3.15}{138}{We show the learning curves for the enhanced Q-learning agents via bandit methods. On the upper plot we despict $\Rt $, the agent's success rate per episode, whereas on the bottom plot we despict $\Pt $, the success probability of the agent's recommended actions at episode $t$, $\llaves {a_{\ell }^{(t)*}}$. Each of the learning curves is averaged over 48 agents; the amplitude was fixed to $\alpha = 0.4$.\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Noise robusteness}{138}{subsection.3.3.5}\protected@file@percent }
\newlabel{ssec:rlcoh_noise}{{3.3.5}{138}{Noise robusteness}{subsection.3.3.5}{}}
\citation{bilkisitw}
\abx@aux@cite{0}{bilkisitw}
\abx@aux@segm{0}{0}{bilkisitw}
\citation{Dequal2020}
\abx@aux@cite{0}{Dequal2020}
\abx@aux@segm{0}{0}{Dequal2020}
\citation{Andrews2005}
\abx@aux@cite{0}{Andrews2005}
\abx@aux@segm{0}{0}{Andrews2005}
\citation{Usenko2012a}
\abx@aux@cite{0}{Usenko2012a}
\abx@aux@segm{0}{0}{Usenko2012a}
\citation{Pirandola2021}
\abx@aux@cite{0}{Pirandola2021}
\abx@aux@segm{0}{0}{Pirandola2021}
\citation{Pirandola2021a}
\abx@aux@cite{0}{Pirandola2021a}
\abx@aux@segm{0}{0}{Pirandola2021a}
\citation{Vasylyev2011}
\abx@aux@cite{0}{Vasylyev2011}
\abx@aux@segm{0}{0}{Vasylyev2011}
\citation{Vasylyev2017}
\abx@aux@cite{0}{Vasylyev2017}
\abx@aux@segm{0}{0}{Vasylyev2017}
\@writefile{toc}{\contentsline {subsubsection}{Compound lossy channels}{139}{subsubsection*.46}\protected@file@percent }
\citation{Dequal2020}
\abx@aux@cite{0}{Dequal2020}
\abx@aux@segm{0}{0}{Dequal2020}
\citation{Andrews2005}
\abx@aux@cite{0}{Andrews2005}
\abx@aux@segm{0}{0}{Andrews2005}
\citation{Usenko2012a}
\abx@aux@cite{0}{Usenko2012a}
\abx@aux@segm{0}{0}{Usenko2012a}
\citation{Pirandola2021}
\abx@aux@cite{0}{Pirandola2021}
\abx@aux@segm{0}{0}{Pirandola2021}
\citation{Pirandola2021a}
\abx@aux@cite{0}{Pirandola2021a}
\abx@aux@segm{0}{0}{Pirandola2021a}
\citation{Vasylyev2011}
\abx@aux@cite{0}{Vasylyev2011}
\abx@aux@segm{0}{0}{Vasylyev2011}
\citation{Vasylyev2017}
\abx@aux@cite{0}{Vasylyev2017}
\abx@aux@segm{0}{0}{Vasylyev2017}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces We compare the performance of the three RL agents (the same considered in Sec.~\ref  {ssec:rlcoh_dolinar_plus_bandit}) at episode $t= 5\cdot 10^{5}$, for several photo-detection noise values (dark count rates). The amplitude of the coherent states is fixed to $\alpha = 0.4$; all data points are averaged over 48 agents. \relax }}{140}{figure.caption.44}\protected@file@percent }
\newlabel{fig:dkresu}{{3.16}{140}{We compare the performance of the three RL agents (the same considered in Sec.~\ref {ssec:rlcoh_dolinar_plus_bandit}) at episode $t= 5\cdot 10^{5}$, for several photo-detection noise values (dark count rates). The amplitude of the coherent states is fixed to $\alpha = 0.4$; all data points are averaged over 48 agents. \relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces We compare the performance of the three RL agents (the same considered in Sec.~\ref  {ssec:rlcoh_dolinar_plus_bandit}) at episode $t= 5\cdot 10^{5}$, as a probability of the signal being phase flipped before arriving to the receiver. The amplitude of the coherent states is fixed to $\alpha = 0.4$; all data points are averaged over 24 agents.\relax }}{141}{figure.caption.45}\protected@file@percent }
\newlabel{fig:dfResults}{{3.17}{141}{We compare the performance of the three RL agents (the same considered in Sec.~\ref {ssec:rlcoh_dolinar_plus_bandit}) at episode $t= 5\cdot 10^{5}$, as a probability of the signal being phase flipped before arriving to the receiver. The amplitude of the coherent states is fixed to $\alpha = 0.4$; all data points are averaged over 24 agents.\relax }{figure.caption.45}{}}
\newlabel{eq:meanps}{{3.54}{141}{Compound lossy channels}{equation.3.3.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces We show Wigner function of inputs and outputs states of the compound channel. We observe that each input state is splitted accoriding to the possible attenuation value.\relax }}{142}{figure.caption.47}\protected@file@percent }
\newlabel{fig:wignercomp}{{3.18}{142}{We show Wigner function of inputs and outputs states of the compound channel. We observe that each input state is splitted accoriding to the possible attenuation value.\relax }{figure.caption.47}{}}
\newlabel{eq:pscom}{{3.56}{142}{Compound lossy channels}{equation.3.3.56}{}}
\newlabel{eq:homodynecompo}{{3.57}{142}{Compound lossy channels}{equation.3.3.57}{}}
\citation{Vasylyev2017}
\abx@aux@cite{0}{Vasylyev2017}
\abx@aux@segm{0}{0}{Vasylyev2017}
\citation{dual_annealing}
\abx@aux@cite{0}{dual_annealing}
\abx@aux@segm{0}{0}{dual_annealing}
\citation{scipy}
\abx@aux@cite{0}{scipy}
\abx@aux@segm{0}{0}{scipy}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Top: We compare the success probabilities attainable by one $(L=1)$ and two ($L=2$) adaptive receivers with homodyne measurement and Helstrom bound, for different values of signal intensity. Bottom: difference between the success probabilities of the receivers under considerations and the Helstrom bound are shown in order to ease the visualizaton of different features. In this case, a lossy channel ($\eta =10^{-2}$) acts on the transmitted state with probability of one half, whereas the signal is transmitted through a noise-less channel otherwise. \relax }}{144}{figure.caption.48}\protected@file@percent }
\newlabel{fig:benchmark}{{3.19}{144}{Top: We compare the success probabilities attainable by one $(L=1)$ and two ($L=2$) adaptive receivers with homodyne measurement and Helstrom bound, for different values of signal intensity. Bottom: difference between the success probabilities of the receivers under considerations and the Helstrom bound are shown in order to ease the visualizaton of different features. In this case, a lossy channel ($\eta =10^{-2}$) acts on the transmitted state with probability of one half, whereas the signal is transmitted through a noise-less channel otherwise. \relax }{figure.caption.48}{}}
\citation{doya}
\abx@aux@cite{0}{doya}
\abx@aux@segm{0}{0}{doya}
\citation{cagata}
\abx@aux@cite{0}{cagata}
\abx@aux@segm{0}{0}{cagata}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces We show learning curve behaviour for $\epsilon $-greedy Q-learning, with an $\epsilon $ exponentially decaying as a function of episode number. The figures of merit are averaged over $48$ agents and corresponding uncertainty region are shown.\relax }}{145}{figure.caption.49}\protected@file@percent }
\newlabel{fig:epgreedycomp}{{3.20}{145}{We show learning curve behaviour for $\epsilon $-greedy Q-learning, with an $\epsilon $ exponentially decaying as a function of episode number. The figures of merit are averaged over $48$ agents and corresponding uncertainty region are shown.\relax }{figure.caption.49}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Discussion and future perspectives}{145}{section.3.4}\protected@file@percent }
\newlabel{ssec:rlcoh_outlook}{{3.4}{145}{Discussion and future perspectives}{section.3.4}{}}
\citation{joseTFG}
\abx@aux@cite{0}{joseTFG}
\abx@aux@segm{0}{0}{joseTFG}
\citation{DDPGpaper}
\abx@aux@cite{0}{DDPGpaper}
\abx@aux@segm{0}{0}{DDPGpaper}
\citation{Jumper2021}
\abx@aux@cite{0}{Jumper2021}
\abx@aux@segm{0}{0}{Jumper2021}
\citation{Schuff_2020}
\abx@aux@cite{0}{Schuff_2020}
\abx@aux@segm{0}{0}{Schuff_2020}
\citation{PScomm}
\abx@aux@cite{0}{PScomm}
\abx@aux@segm{0}{0}{PScomm}
\citation{Cerezo2021}
\abx@aux@cite{0}{Cerezo2021}
\abx@aux@segm{0}{0}{Cerezo2021}
\citation{foselgoogleRL}
\abx@aux@cite{0}{foselgoogleRL}
\abx@aux@segm{0}{0}{foselgoogleRL}
\citation{Carleo2016}
\abx@aux@cite{0}{Carleo2016}
\abx@aux@segm{0}{0}{Carleo2016}
\citation{Torlai2016}
\abx@aux@cite{0}{Torlai2016}
\abx@aux@segm{0}{0}{Torlai2016}
\citation{VanNieuwenburg2017}
\abx@aux@cite{0}{VanNieuwenburg2017}
\abx@aux@segm{0}{0}{VanNieuwenburg2017}
\citation{Carrasquilla2017}
\abx@aux@cite{0}{Carrasquilla2017}
\abx@aux@segm{0}{0}{Carrasquilla2017}
\citation{Torlai2018}
\abx@aux@cite{0}{Torlai2018}
\abx@aux@segm{0}{0}{Torlai2018}
\citation{Melnikov2018}
\abx@aux@cite{0}{Melnikov2018}
\abx@aux@segm{0}{0}{Melnikov2018}
\citation{Fosel2018}
\abx@aux@cite{0}{Fosel2018}
\abx@aux@segm{0}{0}{Fosel2018}
\citation{Wallnofer2019}
\abx@aux@cite{0}{Wallnofer2019}
\abx@aux@segm{0}{0}{Wallnofer2019}
\citation{Bukov2017}
\abx@aux@cite{0}{Bukov2017}
\abx@aux@segm{0}{0}{Bukov2017}
\citation{Niu2019}
\abx@aux@cite{0}{Niu2019}
\abx@aux@segm{0}{0}{Niu2019}
\citation{marek}
\abx@aux@cite{0}{marek}
\abx@aux@segm{0}{0}{marek}
\citation{deeper}
\abx@aux@cite{0}{deeper}
\abx@aux@segm{0}{0}{deeper}
\citation{raultfg}
\abx@aux@cite{0}{raultfg}
\abx@aux@segm{0}{0}{raultfg}
\citation{dynamo}
\abx@aux@cite{0}{dynamo}
\abx@aux@segm{0}{0}{dynamo}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Code}{147}{subsection.3.4.1}\protected@file@percent }
\citation{9363511}
\abx@aux@cite{0}{9363511}
\abx@aux@segm{0}{0}{9363511}
\citation{ShlezingerNir2020VADL}
\abx@aux@cite{0}{ShlezingerNir2020VADL}
\abx@aux@segm{0}{0}{ShlezingerNir2020VADL}
\citation{CNN}
\abx@aux@cite{0}{CNN}
\abx@aux@segm{0}{0}{CNN}
\citation{CNN1}
\abx@aux@cite{0}{CNN1}
\abx@aux@segm{0}{0}{CNN1}
\citation{CNN2}
\abx@aux@cite{0}{CNN2}
\abx@aux@segm{0}{0}{CNN2}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Learning in the twilight}{149}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:VANS}{{4}{149}{Learning in the twilight}{chapter.4}{}}
\citation{bilkis2021semi}
\abx@aux@cite{0}{bilkis2021semi}
\abx@aux@segm{0}{0}{bilkis2021semi}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces {\small  We show a schematic diagram of the VAns algorithm. \textit  {(a)} VAns explores the hyperspace of architectures of parametrized quantum circuits to create short depth ansatzes for VQA applications. VAns takes a (potentially non-trivial) initial circuit (step I) and optimizes its continuous parameters $\bm  {\theta }$ until convergence. At each step, VAns inserts blocks of gates into the circuit which are initialized to the identity (indicated in a box in the figure), so that the ansatzes at contiguous steps belong to an equivalence class of circuits leading to the same cost value (step II). VAns then employs a classical algorithm to simplify the circuit by eliminating gates and finding the shortest circuit (step II to III). The ovals represent subspaces of the architecture hyperspace connected through VAns. While some regions may be smoothly connected by placing identity resolutions, VAns can also explore regions that are not smoothly connected via a gate-simplification process. VAns can either reject (step IV) or accept (step V) modifications in the circuit structure. Here $Z$ ($X$) indicates a rotation about the $z$ ($x$) axis. \textit  {(b)} Schematic representation of the cost function value versus the number of iterations for a typical VAns implementation which follows the steps in \textit  {(a)}.}\relax }}{151}{figure.caption.50}\protected@file@percent }
\newlabel{fig:schematic}{{4.1}{151}{{\small We show a schematic diagram of the VAns algorithm. \textit {(a)} VAns explores the hyperspace of architectures of parametrized quantum circuits to create short depth ansatzes for VQA applications. VAns takes a (potentially non-trivial) initial circuit (step I) and optimizes its continuous parameters $\thv $ until convergence. At each step, VAns inserts blocks of gates into the circuit which are initialized to the identity (indicated in a box in the figure), so that the ansatzes at contiguous steps belong to an equivalence class of circuits leading to the same cost value (step II). VAns then employs a classical algorithm to simplify the circuit by eliminating gates and finding the shortest circuit (step II to III). The ovals represent subspaces of the architecture hyperspace connected through VAns. While some regions may be smoothly connected by placing identity resolutions, VAns can also explore regions that are not smoothly connected via a gate-simplification process. VAns can either reject (step IV) or accept (step V) modifications in the circuit structure. Here $Z$ ($X$) indicates a rotation about the $z$ ($x$) axis. \textit {(b)} Schematic representation of the cost function value versus the number of iterations for a typical VAns implementation which follows the steps in \textit {(a)}.}\relax }{figure.caption.50}{}}
\citation{rattew2019domain}
\abx@aux@cite{0}{rattew2019domain}
\abx@aux@segm{0}{0}{rattew2019domain}
\citation{chivilikhin2020mog}
\abx@aux@cite{0}{chivilikhin2020mog}
\abx@aux@segm{0}{0}{chivilikhin2020mog}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}The Variable Ansatz (VAns) Algorithm}{152}{section.4.1}\protected@file@percent }
\newlabel{sec:vans_vans}{{4.1}{152}{The Variable Ansatz (VAns) Algorithm}{section.4.1}{}}
\newlabel{eq:costbis}{{4.1}{152}{The Variable Ansatz (VAns) Algorithm}{equation.4.1.1}{}}
\citation{rattew2019domain}
\abx@aux@cite{0}{rattew2019domain}
\abx@aux@segm{0}{0}{rattew2019domain}
\citation{maslov2008quantum}
\abx@aux@cite{0}{maslov2008quantum}
\abx@aux@segm{0}{0}{maslov2008quantum}
\citation{hastings1970monte}
\abx@aux@cite{0}{hastings1970monte}
\abx@aux@segm{0}{0}{hastings1970monte}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}\texttt  {Insertion} method}{155}{subsection.4.1.1}\protected@file@percent }
\newlabel{ssec:insertion}{{4.1.1}{155}{\texttt {Insertion} method}{subsection.4.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces We show the two words from the dictionary $\mathcal  {D}$ used during the \texttt  {Insertion} steps. Here we show two types of the parametrized gate sequences composed of CNOTs and rotations about the $z$ and $x$ axis. Specifically, one obtains the identity if the rotation angles are set to zero. Using the circuit in \textit  {(a)}, one inserts a general unitary acting on a given qubit, while the circuit in \textit  {(b)} correlates the two qubits it acts upon.\relax }}{156}{figure.caption.51}\protected@file@percent }
\newlabel{fig:blocks}{{4.2}{156}{We show the two words from the dictionary $\DC $ used during the \texttt {Insertion} steps. Here we show two types of the parametrized gate sequences composed of CNOTs and rotations about the $z$ and $x$ axis. Specifically, one obtains the identity if the rotation angles are set to zero. Using the circuit in \textit {(a)}, one inserts a general unitary acting on a given qubit, while the circuit in \textit {(b)} correlates the two qubits it acts upon.\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}\texttt  {Simplification} method}{156}{subsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces We depict the rules for the \texttt  {Simplification} steps. \textit  {(a)} Commutation rules used by VAns to move gates in the circuit. As shown, we can commute a CNOT with a rotation $Z$ ($X$) about the $z$ ($x$) axis acting on the control (target) qubit; when possible, CNOTs are moved to the left-side of the circuit, and rotations to the right one. \textit  {(b)} Example of simplification rules used by VAns to reduce the circuit depth. Here we assume that the circuit is initialized to $\ket {0}^{\otimes n}$. \relax }}{157}{figure.caption.52}\protected@file@percent }
\newlabel{fig:simplification}{{4.3}{157}{We depict the rules for the \texttt {Simplification} steps. \textit {(a)} Commutation rules used by VAns to move gates in the circuit. As shown, we can commute a CNOT with a rotation $Z$ ($X$) about the $z$ ($x$) axis acting on the control (target) qubit; when possible, CNOTs are moved to the left-side of the circuit, and rotations to the right one. \textit {(b)} Example of simplification rules used by VAns to reduce the circuit depth. Here we assume that the circuit is initialized to $\ket {0}^{\otimes n}$. \relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces We show a non-trivial circuit structure that can be obtained by VAns using the \texttt  {Insertion} and \texttt  {Simplification} steps and the gate dictionary in Fig.~\ref  {fig:blocks}. \relax }}{158}{figure.caption.53}\protected@file@percent }
\newlabel{fig:new}{{4.4}{158}{We show a non-trivial circuit structure that can be obtained by VAns using the \texttt {Insertion} and \texttt {Simplification} steps and the gate dictionary in Fig.~\ref {fig:blocks}. \relax }{figure.caption.53}{}}
\newlabel{alg:algovans}{{\caption@xref {alg:algovans}{ on input line 80}}{159}{\texttt {Simplification} method}{algocf.caption.54}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Pseudo-code for VAns\relax }}{159}{algocf.5}\protected@file@percent }
\citation{peruzzo2014variational}
\abx@aux@cite{0}{peruzzo2014variational}
\abx@aux@segm{0}{0}{peruzzo2014variational}
\citation{romero2017quantum}
\abx@aux@cite{0}{romero2017quantum}
\abx@aux@segm{0}{0}{romero2017quantum}
\citation{broughton2020tensorflow}
\abx@aux@cite{0}{broughton2020tensorflow}
\abx@aux@segm{0}{0}{broughton2020tensorflow}
\citation{kingma2015adam}
\abx@aux@cite{0}{kingma2015adam}
\abx@aux@segm{0}{0}{kingma2015adam}
\citation{qFactor}
\abx@aux@cite{0}{qFactor}
\abx@aux@segm{0}{0}{qFactor}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Scaling of VAns}{160}{subsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Using VAns}{160}{section.4.2}\protected@file@percent }
\newlabel{ssec:vans_results}{{4.2}{160}{Using VAns}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Transverse Field Ising model (TFIM)}{161}{subsection.4.2.1}\protected@file@percent }
\newlabel{eq:HTFIM}{{4.2}{161}{Transverse Field Ising model (TFIM)}{equation.4.2.2}{}}
\newlabel{eq:}{{4.3}{161}{Transverse Field Ising model (TFIM)}{equation.4.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Results of using VAns to obtain the ground state of a Transverse Field Ising model: we use VAns in the VQE algorithm for the Hamiltonian in Eq.~\ref  {eq:HTFIM} with \textit  {(a)} $n=4$ qubits and \textit  {(b)} $n=8$ qubits, field $g=1$, for different values of the interaction $J$. Top panels: solid lines indicate the exact ground state energy, and the markers are the energies obtained using VAns. Bottom panels: Relative error in the energy for the same interaction values.\relax }}{162}{figure.caption.55}\protected@file@percent }
\newlabel{fig:TFIM}{{4.5}{162}{Results of using VAns to obtain the ground state of a Transverse Field Ising model: we use VAns in the VQE algorithm for the Hamiltonian in Eq.~\ref {eq:HTFIM} with \textit {(a)} $n=4$ qubits and \textit {(b)} $n=8$ qubits, field $g=1$, for different values of the interaction $J$. Top panels: solid lines indicate the exact ground state energy, and the markers are the energies obtained using VAns. Bottom panels: Relative error in the energy for the same interaction values.\relax }{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces We show VAns learning process: \textit  {(a)} we show an instance of running the algorithm for the Hamiltonian in Eq.~\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:HTFIM}\unskip \@@italiccorr )}} with $n=8$ qubits, field $g=1$, and interaction $J=1.5$. The top panel shows the cost function value and the bottom panel depicts the number of CNOTs, and the number of trainable parameters versus the number of modifications of the ansatz accepted in the VAns algorithm. Top: As the number of iterations increases, VAns minimizes the energy until one finds the ground state of the TFIM. Here we also show the best results obtained by training a fixed structure layered Hardware Efficient Ansatz (HEA) with $2$ and $5$ layers, and in both cases, VAns outperforms the HEA. Bottom: While initially the number of CNOTs and number of trainable parameters increases, the \texttt  {Simplification} method in VAns prevents the circuit from constantly growing, and can even lead to shorter depth circuits that achieve better solutions. Here we also show the number of CNOTs (solid line) and parameters (dashed line) in the HEA ansatzes considered, and we see that VAns can obtain circuits with less entangling and trainable gates. \relax }}{163}{figure.caption.56}\protected@file@percent }
\newlabel{fig:learning}{{4.6}{163}{We show VAns learning process: \textit {(a)} we show an instance of running the algorithm for the Hamiltonian in Eq.~\eqref {eq:HTFIM} with $n=8$ qubits, field $g=1$, and interaction $J=1.5$. The top panel shows the cost function value and the bottom panel depicts the number of CNOTs, and the number of trainable parameters versus the number of modifications of the ansatz accepted in the VAns algorithm. Top: As the number of iterations increases, VAns minimizes the energy until one finds the ground state of the TFIM. Here we also show the best results obtained by training a fixed structure layered Hardware Efficient Ansatz (HEA) with $2$ and $5$ layers, and in both cases, VAns outperforms the HEA. Bottom: While initially the number of CNOTs and number of trainable parameters increases, the \texttt {Simplification} method in VAns prevents the circuit from constantly growing, and can even lead to shorter depth circuits that achieve better solutions. Here we also show the number of CNOTs (solid line) and parameters (dashed line) in the HEA ansatzes considered, and we see that VAns can obtain circuits with less entangling and trainable gates. \relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces We show a low-depth, ground-state preparing circuits found by VAns during the learning process; here $Z$ ($X$) indicates a rotation about the $z$ ($x$) axis, about the corresponding value appearing below.\relax }}{164}{figure.caption.57}\protected@file@percent }
\newlabel{fig:circui}{{4.7}{164}{We show a low-depth, ground-state preparing circuits found by VAns during the learning process; here $Z$ ($X$) indicates a rotation about the $z$ ($x$) axis, about the corresponding value appearing below.\relax }{figure.caption.57}{}}
\citation{cerezo2017factorization}
\abx@aux@cite{0}{cerezo2017factorization}
\abx@aux@segm{0}{0}{cerezo2017factorization}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces We show results of using VAns to obtain the ground state of a Heisenberg $XXZ$ model. Here, we consider the VQE algorithm for the Hamiltonian in Eq.~\ref  {eq:HXXZ} with \textit  {(a)} $n=4$ and \textit  {(b)} $n=8$ qubits, field $g=1$, and indicated anisotropies $\Delta $. Top panels: The solid line indicates the exact ground state energy, and the markers are the energies obtained using VAns. Bottom panels: Relative error in the energy for the same anisotropy values.\relax }}{165}{figure.caption.58}\protected@file@percent }
\newlabel{fig:XXZ}{{4.8}{165}{We show results of using VAns to obtain the ground state of a Heisenberg $XXZ$ model. Here, we consider the VQE algorithm for the Hamiltonian in Eq.~\ref {eq:HXXZ} with \textit {(a)} $n=4$ and \textit {(b)} $n=8$ qubits, field $g=1$, and indicated anisotropies $\Delta $. Top panels: The solid line indicates the exact ground state energy, and the markers are the energies obtained using VAns. Bottom panels: Relative error in the energy for the same anisotropy values.\relax }{figure.caption.58}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}$XXZ$ Heisenberg Model}{165}{subsection.4.2.2}\protected@file@percent }
\newlabel{eq:HXXZ}{{4.4}{165}{$XXZ$ Heisenberg Model}{equation.4.2.4}{}}
\citation{cervera2020meta}
\abx@aux@cite{0}{cervera2020meta}
\abx@aux@segm{0}{0}{cervera2020meta}
\citation{RevModPhys.92.015003}
\abx@aux@cite{0}{RevModPhys.92.015003}
\abx@aux@segm{0}{0}{RevModPhys.92.015003}
\citation{mcclean2019openfermion}
\abx@aux@cite{0}{mcclean2019openfermion}
\abx@aux@segm{0}{0}{mcclean2019openfermion}
\citation{mcardle2020quantum}
\abx@aux@cite{0}{mcardle2020quantum}
\abx@aux@segm{0}{0}{mcardle2020quantum}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Molecular Hamiltonians}{166}{subsection.4.2.3}\protected@file@percent }
\newlabel{ssec:molecular_ham_vans}{{4.2.3}{166}{Molecular Hamiltonians}{subsection.4.2.3}{}}
\citation{mcclean2019openfermion}
\abx@aux@cite{0}{mcclean2019openfermion}
\abx@aux@segm{0}{0}{mcclean2019openfermion}
\citation{RevModPhys.92.015003}
\abx@aux@cite{0}{RevModPhys.92.015003}
\abx@aux@segm{0}{0}{RevModPhys.92.015003}
\newlabel{eq:Hferm}{{4.5}{167}{Molecular Hamiltonians}{equation.4.2.5}{}}
\citation{mcclean2019openfermion}
\abx@aux@cite{0}{mcclean2019openfermion}
\abx@aux@segm{0}{0}{mcclean2019openfermion}
\citation{mcardle2020quantum}
\abx@aux@cite{0}{mcardle2020quantum}
\abx@aux@segm{0}{0}{mcardle2020quantum}
\newlabel{eq:}{{4.9}{168}{Molecular Hamiltonians}{equation.4.2.9}{}}
\newlabel{eq:HfermJW}{{4.10}{168}{Molecular Hamiltonians}{equation.4.2.10}{}}
\newlabel{fig:h22}{{4.9a}{169}{\relax }{figure.caption.59}{}}
\newlabel{sub@fig:h22}{{a}{169}{\relax }{figure.caption.59}{}}
\newlabel{fig:h24}{{4.9b}{169}{\relax }{figure.caption.59}{}}
\newlabel{sub@fig:h24}{{b}{169}{\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces We show results of using VAns to obtain the ground state of a Hydrogen (H4) molecule, at different bond lengths (for the H4 molecule, this means the distance between the equally-separated atoms, which are disposed in a lineal array). Here we use VAns in the VQE algorithm for the molecular Hamiltonian obtained after a Jordan-Wigner transformation, leading to a 4(8)-qubit circuit in left(right) panels. Top: Solid lines correspond to ground state energy as computed by the Full Configuration Interaction (FCI) method, whereas points correspond to energies obtained using VAns. Middle: Differences between exact and VAns ground state energies are shown. Dashed line corresponds to chemical accuracy, which stands for the ultimate accuracy experimentally reachable in such systems. Bottom: Number of iterations required by VAns until convergence are shown.\relax }}{169}{figure.caption.59}\protected@file@percent }
\newlabel{fig:H4}{{4.9}{169}{We show results of using VAns to obtain the ground state of a Hydrogen (H4) molecule, at different bond lengths (for the H4 molecule, this means the distance between the equally-separated atoms, which are disposed in a lineal array). Here we use VAns in the VQE algorithm for the molecular Hamiltonian obtained after a Jordan-Wigner transformation, leading to a 4(8)-qubit circuit in left(right) panels. Top: Solid lines correspond to ground state energy as computed by the Full Configuration Interaction (FCI) method, whereas points correspond to energies obtained using VAns. Middle: Differences between exact and VAns ground state energies are shown. Dashed line corresponds to chemical accuracy, which stands for the ultimate accuracy experimentally reachable in such systems. Bottom: Number of iterations required by VAns until convergence are shown.\relax }{figure.caption.59}{}}
\citation{romero2017quantum}
\abx@aux@cite{0}{romero2017quantum}
\abx@aux@segm{0}{0}{romero2017quantum}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Schematic diagram of the quantum autoencoder implementation. We first employ VAns to learn the circuits that prepare the ground states $\{\ket {\psi _i}\}_{i=1}^M$ of the $H_2$ molecule for different bond lengths. These ground states are then used to create a training set and test set for the quantum autoencoder implementation. The goal of the autoencoder is to train an encoding parametrized quantum circuit $V(\mathbf  {k}, \bm  {\theta })$ to compress each $\ket {\psi _i}$ into a subsystem of two qubits so that one can recover $\ket {\psi _i}$ from the reduced states, and we quantify this using the cost in Eq.~\ref  {eq:costautoencoderglobal}.\relax }}{170}{figure.caption.60}\protected@file@percent }
\newlabel{fig:AE}{{4.10}{170}{Schematic diagram of the quantum autoencoder implementation. We first employ VAns to learn the circuits that prepare the ground states $\{\ket {\psi _i}\}_{i=1}^M$ of the $H_2$ molecule for different bond lengths. These ground states are then used to create a training set and test set for the quantum autoencoder implementation. The goal of the autoencoder is to train an encoding parametrized quantum circuit $V(\kvec , \thv )$ to compress each $\ket {\psi _i}$ into a subsystem of two qubits so that one can recover $\ket {\psi _i}$ from the reduced states, and we quantify this using the cost in Eq.~\ref {eq:costautoencoderglobal}.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Quantum Autoencoder}{170}{subsection.4.2.4}\protected@file@percent }
\citation{romero2017quantum}
\abx@aux@cite{0}{romero2017quantum}
\abx@aux@segm{0}{0}{romero2017quantum}
\citation{romero2017quantum}
\abx@aux@cite{0}{romero2017quantum}
\abx@aux@segm{0}{0}{romero2017quantum}
\newlabel{eq:costautoencoderglobal}{{4.13}{171}{Quantum Autoencoder}{equation.4.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces  {\small  Results of using VAns to train a quantum autoencoder. Here we use VAns to train an encoding parametrized quantum circuit by minimizing Eq.~\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:costautoencoderglobal}\unskip \@@italiccorr )}} on a training set comprised of six ground states of the hydrogen molecule. We here also show the lowest cost function obtained for a $L$-HEA of $L=4$ and $L=15$ layers. Top panel: the cost function evaluated at both versus accepted VAns circuit modifications. In addition, we also show results of evaluating the cost on the testing set. Bottom panel: number of CNOTs, and number of trainable parameters versus the number of modifications of the ansatz accepted in the VAns algorithm. Here we additionally show the number of CNOTs (solid line) and parameters (dashed line) in the HEA ansatzes considered. We remark that for 15-HEA has $180$ parameters, and hence the curve is not shown as it would be off the scale.} \relax }}{172}{figure.caption.61}\protected@file@percent }
\newlabel{fig:AE_results}{{4.11}{172}{{\small Results of using VAns to train a quantum autoencoder. Here we use VAns to train an encoding parametrized quantum circuit by minimizing Eq.~\eqref {eq:costautoencoderglobal} on a training set comprised of six ground states of the hydrogen molecule. We here also show the lowest cost function obtained for a $L$-HEA of $L=4$ and $L=15$ layers. Top panel: the cost function evaluated at both versus accepted VAns circuit modifications. In addition, we also show results of evaluating the cost on the testing set. Bottom panel: number of CNOTs, and number of trainable parameters versus the number of modifications of the ansatz accepted in the VAns algorithm. Here we additionally show the number of CNOTs (solid line) and parameters (dashed line) in the HEA ansatzes considered. We remark that for 15-HEA has $180$ parameters, and hence the curve is not shown as it would be off the scale.} \relax }{figure.caption.61}{}}
\citation{caro2021generalization}
\abx@aux@cite{0}{caro2021generalization}
\abx@aux@segm{0}{0}{caro2021generalization}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Unitary compilation}{173}{subsection.4.2.5}\protected@file@percent }
\newlabel{sec:unitary_compilation}{{4.2.5}{173}{Unitary compilation}{subsection.4.2.5}{}}
\newlabel{eq:UC_training_set}{{4.14}{173}{Unitary compilation}{equation.4.2.14}{}}
\newlabel{eq:UC_cost}{{4.15}{173}{Unitary compilation}{equation.4.2.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Results of using VAns for unitary compilation. Here we use VAns to find a decomposition of QFT unitary defined on $n=10$ qubits, by minimizing a cost function $C(\mathbf  {k},\bm  {\theta })$ (red line in panel \textit  {(a)} defined in Eq.~\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:UC_cost}\unskip \@@italiccorr )}}. The cost evaluates a difference between exact output of QFT and the one returned by a current circuit, on a small number of input states only ($M=15$). The blue line shows corresponding difference between full unitaries, $C'(\mathbf  {k},\bm  {\theta }) = || U_\mathrm  {QFT}^{(n)} - V(\mathbf  {k},\bm  {\theta })||^2$. We observe a high correlation between those two cost functions. Panel \textit  {(b)} shows how VAns modifies the number of two-qubit gates as it approaches the minimum of $C(\mathbf  {k},\bm  {\theta })$. The minimum is found with 48 gates, which is $\sim 4.5$ times less than the decomposition found with HEA (not shown). \relax }}{175}{figure.caption.62}\protected@file@percent }
\newlabel{fig:unitary_compilation_results}{{4.12}{175}{Results of using VAns for unitary compilation. Here we use VAns to find a decomposition of QFT unitary defined on $n=10$ qubits, by minimizing a cost function $C(\kvec ,\thv )$ (red line in panel \textit {(a)} defined in Eq.~\eqref {eq:UC_cost}. The cost evaluates a difference between exact output of QFT and the one returned by a current circuit, on a small number of input states only ($M=15$). The blue line shows corresponding difference between full unitaries, $C'(\kvec ,\thv ) = || U_\mathrm {QFT}^{(n)} - V(\kvec ,\thv )||^2$. We observe a high correlation between those two cost functions. Panel \textit {(b)} shows how VAns modifies the number of two-qubit gates as it approaches the minimum of $C(\kvec ,\thv )$. The minimum is found with 48 gates, which is $\sim 4.5$ times less than the decomposition found with HEA (not shown). \relax }{figure.caption.62}{}}
\citation{caro2021generalization}
\abx@aux@cite{0}{caro2021generalization}
\abx@aux@segm{0}{0}{caro2021generalization}
\citation{Georgopoulos2021Modeling}
\abx@aux@cite{0}{Georgopoulos2021Modeling}
\abx@aux@segm{0}{0}{Georgopoulos2021Modeling}
\citation{larose2022error}
\abx@aux@cite{0}{larose2022error}
\abx@aux@segm{0}{0}{larose2022error}
\citation{Prakash2020Software}
\abx@aux@cite{0}{Prakash2020Software}
\abx@aux@segm{0}{0}{Prakash2020Software}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Noise and the $\lambda $-model}{176}{subsection.4.2.6}\protected@file@percent }
\newlabel{ssec:vans_results_noise}{{4.2.6}{176}{Noise and the $\lambda $-model}{subsection.4.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Results of using VAns for VQE under the $\lambda $-model. Here we consider the TFIM for 8 qubits, with $g=J=1$. The results are obtained after repeating 50 iterations of optimizations with VAns and HEA respectively. We observe that VAns discovers much more efficient quantum circuits as compared to HEA. As shown in the upper inset, VAns automatically adjusts the circuit layout according to the noise strength at hand, a feature that fix-structure ansatzes lack. In the lower inset we show the relative errors (\textit  {e.g.} standard deviation over optimal cost found) for both ansatzes, across the 50 iterations; we observe that VAns is more precise in reaching a minimum as compared to HEA. We note that in this experiment we have initialized VAns to a 1-HEA, which is in turn inconvenient for a sufficiently high value of $\lambda $. Yet, VAns learns how to adapt the ansatz (in this case, finding a separable one) so to reach the lowest cost value. In all cases VAns termination criteria was set to a maximum number of 30 iterations.\relax }}{177}{figure.caption.63}\protected@file@percent }
\newlabel{fig:Fig13}{{4.13}{177}{Results of using VAns for VQE under the $\lambda $-model. Here we consider the TFIM for 8 qubits, with $g=J=1$. The results are obtained after repeating 50 iterations of optimizations with VAns and HEA respectively. We observe that VAns discovers much more efficient quantum circuits as compared to HEA. As shown in the upper inset, VAns automatically adjusts the circuit layout according to the noise strength at hand, a feature that fix-structure ansatzes lack. In the lower inset we show the relative errors (\textit {e.g.} standard deviation over optimal cost found) for both ansatzes, across the 50 iterations; we observe that VAns is more precise in reaching a minimum as compared to HEA. We note that in this experiment we have initialized VAns to a 1-HEA, which is in turn inconvenient for a sufficiently high value of $\lambda $. Yet, VAns learns how to adapt the ansatz (in this case, finding a separable one) so to reach the lowest cost value. In all cases VAns termination criteria was set to a maximum number of 30 iterations.\relax }{figure.caption.63}{}}
\citation{Obrien2004Quantum}
\abx@aux@cite{0}{Obrien2004Quantum}
\abx@aux@segm{0}{0}{Obrien2004Quantum}
\citation{Zhou2014Process}
\abx@aux@cite{0}{Zhou2014Process}
\abx@aux@segm{0}{0}{Zhou2014Process}
\citation{Blank2020Quantum}
\abx@aux@cite{0}{Blank2020Quantum}
\abx@aux@segm{0}{0}{Blank2020Quantum}
\citation{Georgopoulos2021Modeling}
\abx@aux@cite{0}{Georgopoulos2021Modeling}
\abx@aux@segm{0}{0}{Georgopoulos2021Modeling}
\citation{supercomi}
\abx@aux@cite{0}{supercomi}
\abx@aux@segm{0}{0}{supercomi}
\citation{foselgoogleRL}
\abx@aux@cite{0}{foselgoogleRL}
\abx@aux@segm{0}{0}{foselgoogleRL}
\citation{Moro2021}
\abx@aux@cite{0}{Moro2021}
\abx@aux@segm{0}{0}{Moro2021}
\citation{HerreraMarti2022policygradient}
\abx@aux@cite{0}{HerreraMarti2022policygradient}
\abx@aux@segm{0}{0}{HerreraMarti2022policygradient}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Discussion and future perspectives}{179}{section.4.3}\protected@file@percent }
\newlabel{ssec:vans_discu}{{4.3}{179}{Discussion and future perspectives}{section.4.3}{}}
\citation{bravo2020variational}
\abx@aux@cite{0}{bravo2020variational}
\abx@aux@segm{0}{0}{bravo2020variational}
\citation{huang2019near}
\abx@aux@cite{0}{huang2019near}
\abx@aux@segm{0}{0}{huang2019near}
\citation{xu2019variational}
\abx@aux@cite{0}{xu2019variational}
\abx@aux@segm{0}{0}{xu2019variational}
\citation{anschuetz2019variational}
\abx@aux@cite{0}{anschuetz2019variational}
\abx@aux@segm{0}{0}{anschuetz2019variational}
\citation{khatri2019quantum}
\abx@aux@cite{0}{khatri2019quantum}
\abx@aux@segm{0}{0}{khatri2019quantum}
\citation{sharma2019noise}
\abx@aux@cite{0}{sharma2019noise}
\abx@aux@segm{0}{0}{sharma2019noise}
\citation{beckey2020variational}
\abx@aux@cite{0}{beckey2020variational}
\abx@aux@segm{0}{0}{beckey2020variational}
\citation{koczor2020variational}
\abx@aux@cite{0}{koczor2020variational}
\abx@aux@segm{0}{0}{koczor2020variational}
\citation{larose2019variational}
\abx@aux@cite{0}{larose2019variational}
\abx@aux@segm{0}{0}{larose2019variational}
\citation{cerezo2020variational}
\abx@aux@cite{0}{cerezo2020variational}
\abx@aux@segm{0}{0}{cerezo2020variational}
\citation{biamonte2017quantum}
\abx@aux@cite{0}{biamonte2017quantum}
\abx@aux@segm{0}{0}{biamonte2017quantum}
\citation{schuld2014quest}
\abx@aux@cite{0}{schuld2014quest}
\abx@aux@segm{0}{0}{schuld2014quest}
\citation{abbas2020power}
\abx@aux@cite{0}{abbas2020power}
\abx@aux@segm{0}{0}{abbas2020power}
\citation{verdon2019quantum}
\abx@aux@cite{0}{verdon2019quantum}
\abx@aux@segm{0}{0}{verdon2019quantum}
\citation{mcclean2018barren}
\abx@aux@cite{0}{mcclean2018barren}
\abx@aux@segm{0}{0}{mcclean2018barren}
\citation{brandao2016local}
\abx@aux@cite{0}{brandao2016local}
\abx@aux@segm{0}{0}{brandao2016local}
\citation{dankert2009exact}
\abx@aux@cite{0}{dankert2009exact}
\abx@aux@segm{0}{0}{dankert2009exact}
\citation{harrow2009random}
\abx@aux@cite{0}{harrow2009random}
\abx@aux@segm{0}{0}{harrow2009random}
\citation{harrow2018approximate}
\abx@aux@cite{0}{harrow2018approximate}
\abx@aux@segm{0}{0}{harrow2018approximate}
\citation{haferkamp2022randomquantum}
\abx@aux@cite{0}{haferkamp2022randomquantum}
\abx@aux@segm{0}{0}{haferkamp2022randomquantum}
\citation{wang2020noise}
\abx@aux@cite{0}{wang2020noise}
\abx@aux@segm{0}{0}{wang2020noise}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Mitigating the effect of barren plateaus}{180}{subsection.4.3.1}\protected@file@percent }
\citation{sack2022avoiding}
\abx@aux@cite{0}{sack2022avoiding}
\abx@aux@segm{0}{0}{sack2022avoiding}
\citation{sharma2020trainability}
\abx@aux@cite{0}{sharma2020trainability}
\abx@aux@segm{0}{0}{sharma2020trainability}
\citation{patti2020entanglement}
\abx@aux@cite{0}{patti2020entanglement}
\abx@aux@segm{0}{0}{patti2020entanglement}
\citation{marrero2020entanglement}
\abx@aux@cite{0}{marrero2020entanglement}
\abx@aux@segm{0}{0}{marrero2020entanglement}
\citation{sack2022avoiding}
\abx@aux@cite{0}{sack2022avoiding}
\abx@aux@segm{0}{0}{sack2022avoiding}
\citation{huang2020predicting}
\abx@aux@cite{0}{huang2020predicting}
\abx@aux@segm{0}{0}{huang2020predicting}
\citation{sack2022avoiding}
\abx@aux@cite{0}{sack2022avoiding}
\abx@aux@segm{0}{0}{sack2022avoiding}
\citation{grant2019initialization}
\abx@aux@cite{0}{grant2019initialization}
\abx@aux@segm{0}{0}{grant2019initialization}
\citation{sack2022avoiding}
\abx@aux@cite{0}{sack2022avoiding}
\abx@aux@segm{0}{0}{sack2022avoiding}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Future directions}{181}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{BP-aware implementation}{181}{subsubsection*.64}\protected@file@percent }
\newlabel{eq:}{{4.16}{181}{BP-aware implementation}{equation.4.3.16}{}}
\citation{cvnetworks}
\abx@aux@cite{0}{cvnetworks}
\abx@aux@segm{0}{0}{cvnetworks}
\@writefile{toc}{\contentsline {subsubsection}{Reinforcement-learning}{182}{subsubsection*.65}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{CV systems, receivers and beyond}{182}{subsubsection*.66}\protected@file@percent }
\citation{Vansgb0}
\abx@aux@cite{0}{Vansgb0}
\abx@aux@segm{0}{0}{Vansgb0}
\citation{Vansgb}
\abx@aux@cite{0}{Vansgb}
\abx@aux@segm{0}{0}{Vansgb}
\citation{castellers}
\abx@aux@cite{0}{castellers}
\abx@aux@segm{0}{0}{castellers}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Code}{183}{subsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Learning in the daylight}{185}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:CMON}{{5}{185}{Learning in the daylight}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{185}{section.5.1}\protected@file@percent }
\citation{Kiilerich2018Hypothesis}
\abx@aux@cite{0}{Kiilerich2018Hypothesis}
\abx@aux@segm{0}{0}{Kiilerich2018Hypothesis}
\citation{Vargas2021quantum}
\abx@aux@cite{0}{Vargas2021quantum}
\abx@aux@segm{0}{0}{Vargas2021quantum}
\citation{Li2022seq}
\abx@aux@cite{0}{Li2022seq}
\abx@aux@segm{0}{0}{Li2022seq}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Hypothesis testing in continuously-monitored systems}{186}{section.5.2}\protected@file@percent }
\citation{doherty1999feedback}
\abx@aux@cite{0}{doherty1999feedback}
\abx@aux@segm{0}{0}{doherty1999feedback}
\citation{Wiseman2005optimal}
\abx@aux@cite{0}{Wiseman2005optimal}
\abx@aux@segm{0}{0}{Wiseman2005optimal}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\newlabel{eq:CMON_BELAVKIN}{{5.1}{187}{Hypothesis testing in continuously-monitored systems}{equation.5.2.1}{}}
\newlabel{eq:}{{5.4}{187}{Hypothesis testing in continuously-monitored systems}{equation.5.2.4}{}}
\newlabel{eq:gaussmodel}{{5.5}{188}{Hypothesis testing in continuously-monitored systems}{equation.5.2.5}{}}
\newlabel{eq:gaussmodeltot}{{5.7}{188}{Hypothesis testing in continuously-monitored systems}{equation.5.2.7}{}}
\newlabel{eq:dynamics_lambda}{{5.8}{188}{Hypothesis testing in continuously-monitored systems}{equation.5.2.8}{}}
\newlabel{eq:CMONratio}{{5.9}{188}{Hypothesis testing in continuously-monitored systems}{equation.5.2.9}{}}
\newlabel{eq:}{{5.11}{189}{Hypothesis testing in continuously-monitored systems}{equation.5.2.11}{}}
\newlabel{eq:likevK}{{5.12}{189}{Hypothesis testing in continuously-monitored systems}{equation.5.2.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Sequential testing $\&$ Gaussian systems}{189}{section.5.3}\protected@file@percent }
\newlabel{eq:cmon_LINEALSYSTEM}{{5.13}{189}{Sequential testing $\&$ Gaussian systems}{equation.5.3.13}{}}
\citation{Wiseman2005optimal}
\abx@aux@cite{0}{Wiseman2005optimal}
\abx@aux@segm{0}{0}{Wiseman2005optimal}
\newlabel{eq:CMON_signal}{{5.14}{190}{Sequential testing $\&$ Gaussian systems}{equation.5.3.14}{}}
\newlabel{eq:cmon_rica2}{{5.15}{190}{Sequential testing $\&$ Gaussian systems}{equation.5.3.15}{}}
\newlabel{eq:homodyneROT}{{5.16}{190}{Sequential testing $\&$ Gaussian systems}{equation.5.3.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces We show a realization of a quantum trajectory (top) along with the measurement record (bottom), for the process described by Eq.~\ref  {eq:cmon_LINEALSYSTEM}\relax }}{191}{figure.caption.67}\protected@file@percent }
\newlabel{fig:evol}{{5.1}{191}{We show a realization of a quantum trajectory (top) along with the measurement record (bottom), for the process described by Eq.~\ref {eq:cmon_LINEALSYSTEM}\relax }{figure.caption.67}{}}
\citation{doherty1999feedback}
\abx@aux@cite{0}{doherty1999feedback}
\abx@aux@segm{0}{0}{doherty1999feedback}
\newlabel{eq:lambdaGauss}{{5.17}{192}{Sequential testing $\&$ Gaussian systems}{equation.5.3.17}{}}
\newlabel{eq:linealLog}{{5.18}{192}{Sequential testing $\&$ Gaussian systems}{equation.5.3.18}{}}
\citation{Fallani2022Learning}
\abx@aux@cite{0}{Fallani2022Learning}
\abx@aux@segm{0}{0}{Fallani2022Learning}
\citation{scipy}
\abx@aux@cite{0}{scipy}
\abx@aux@segm{0}{0}{scipy}
\citation{schurCARE}
\abx@aux@cite{0}{schurCARE}
\abx@aux@segm{0}{0}{schurCARE}
\@writefile{toc}{\contentsline {subsubsection}{Damping discrimination}{193}{subsubsection*.68}\protected@file@percent }
\newlabel{eq:stat_cov_damping}{{5.19}{193}{Damping discrimination}{equation.5.3.19}{}}
\newlabel{eq:rkstoch}{{5.21}{193}{Damping discrimination}{equation.5.3.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The distributions for the hidden states are shown, simulated under the alternative ($\gamma = 429 $ Hz) and null hypothesis ($\gamma = 100$ Hz) respectively. The histograms are obtained by simulating $2 \cdot 10^{4}$ quantum trajectories, and the time for which the distributions are displayed is $T=4s$.\relax }}{194}{figure.caption.69}\protected@file@percent }
\newlabel{fig:cmon_distributions_hidden}{{5.2}{194}{The distributions for the hidden states are shown, simulated under the alternative ($\gamma = 429 $ Hz) and null hypothesis ($\gamma = 100$ Hz) respectively. The histograms are obtained by simulating $2 \cdot 10^{4}$ quantum trajectories, and the time for which the distributions are displayed is $T=4s$.\relax }{figure.caption.69}{}}
\newlabel{eq:vkst}{{5.22}{194}{Damping discrimination}{equation.5.3.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}The log-likelihood ratio}{194}{subsection.5.3.1}\protected@file@percent }
\newlabel{eq:expec_state_true}{{5.24}{195}{The log-likelihood ratio}{equation.5.3.24}{}}
\newlabel{eq:expect_states_alt}{{5.25}{195}{The log-likelihood ratio}{equation.5.3.25}{}}
\newlabel{eq:dlav}{{5.27}{195}{The log-likelihood ratio}{equation.5.3.27}{}}
\newlabel{eq:mukANAL}{{5.28}{195}{The log-likelihood ratio}{equation.5.3.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces We show the stochastic evolution of the log-likelihood ratio under both models, together with its empirical average computed over $2 \cdot 10^{4}$ trajectories, for the damping discrimination problem considered in this Section.\relax }}{196}{figure.caption.70}\protected@file@percent }
\newlabel{fig:liks_drfit}{{5.3}{196}{We show the stochastic evolution of the log-likelihood ratio under both models, together with its empirical average computed over $2 \cdot 10^{4}$ trajectories, for the damping discrimination problem considered in this Section.\relax }{figure.caption.70}{}}
\newlabel{eq:dint}{{5.29}{196}{The log-likelihood ratio}{equation.5.3.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces We compare the first two moments estimated over $2 \cdot 10^{4}$ trajectories with their predicted theoretical value, for both hypothesis, in the damping discrimination scenario under consideration.\relax }}{197}{figure.caption.71}\protected@file@percent }
\newlabel{fig:cmon_momentos}{{5.4}{197}{We compare the first two moments estimated over $2 \cdot 10^{4}$ trajectories with their predicted theoretical value, for both hypothesis, in the damping discrimination scenario under consideration.\relax }{figure.caption.71}{}}
\newlabel{eq:serr2}{{5.30}{198}{The log-likelihood ratio}{equation.5.3.30}{}}
\newlabel{eq:decisionsASPRT}{{5.32}{198}{The log-likelihood ratio}{equation.5.3.32}{}}
\newlabel{eq:cmon_sprt_weak}{{5.33}{198}{The log-likelihood ratio}{equation.5.3.33}{}}
\newlabel{eq:cmonWlad}{{5.35}{198}{The log-likelihood ratio}{equation.5.3.35}{}}
\newlabel{eq:waldcmon}{{5.40}{199}{The log-likelihood ratio}{equation.5.3.40}{}}
\newlabel{eq:waldcmon3}{{5.41}{199}{The log-likelihood ratio}{equation.5.3.41}{}}
\newlabel{eq:waldcmon2}{{5.42}{199}{The log-likelihood ratio}{equation.5.3.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces We compare the average stopping time (under each hypothesis and also its mean value) with our analytical curve given in Eq.~\ref  {eq:waldcmon}.\relax }}{200}{figure.caption.72}\protected@file@percent }
\newlabel{fig:cmonwald}{{5.5}{200}{We compare the average stopping time (under each hypothesis and also its mean value) with our analytical curve given in Eq.~\ref {eq:waldcmon}.\relax }{figure.caption.72}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}To be or not to be Gaussian}{200}{subsection.5.3.2}\protected@file@percent }
\newlabel{ssec:cmon_gaussian_model}{{5.3.2}{200}{To be or not to be Gaussian}{subsection.5.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces We illustrate how the stopping time distribution arises from the random trajectories of $\ell _{t}$; here we show some realisations of the $\ell $ process, along with some time-slices at $t_0$ and $t_1$ shown in blue together with the corresponding distributions $P(\ell _t)$. The horizontal line in red corresponds to the fixed threshold $\ell =a$, leading to an arrival time distribution $P(\tau )$. Such distribution appears as a consequence of a difference in the arrival times for trajectories of $\ell _{t}$, which are governed by a Gaussian distribution that is drifted and diffuses in time.\relax }}{201}{figure.caption.73}\protected@file@percent }
\newlabel{fig:cmonwald}{{5.6}{201}{We illustrate how the stopping time distribution arises from the random trajectories of $\ell _{t}$; here we show some realisations of the $\ell $ process, along with some time-slices at $t_0$ and $t_1$ shown in blue together with the corresponding distributions $P(\ell _t)$. The horizontal line in red corresponds to the fixed threshold $\ell =a$, leading to an arrival time distribution $P(\tau )$. Such distribution appears as a consequence of a difference in the arrival times for trajectories of $\ell _{t}$, which are governed by a Gaussian distribution that is drifted and diffuses in time.\relax }{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces We show the distributions for the log-likelihood ratio at different times, for both hypothesis, computed over $2\cdot 10^{4}$ for the damping discrimination problem considered in this Section. Here, we have also plotted the distributions associated to the Gaussian model discussed in Sec.\ref  {ssec:cmon_gaussian_model}.\relax }}{202}{figure.caption.74}\protected@file@percent }
\newlabel{fig:histoell}{{5.7}{202}{We show the distributions for the log-likelihood ratio at different times, for both hypothesis, computed over $2\cdot 10^{4}$ for the damping discrimination problem considered in this Section. Here, we have also plotted the distributions associated to the Gaussian model discussed in Sec.\ref {ssec:cmon_gaussian_model}.\relax }{figure.caption.74}{}}
\newlabel{eq:}{{5.44}{202}{To be or not to be Gaussian}{equation.5.3.44}{}}
\newlabel{eq:gausslog}{{5.45}{203}{To be or not to be Gaussian}{equation.5.3.45}{}}
\newlabel{eq:prob_gauss_log}{{5.46}{203}{To be or not to be Gaussian}{equation.5.3.46}{}}
\newlabel{eq:cmon_errors_weak_gaussian}{{5.47}{204}{To be or not to be Gaussian}{equation.5.3.47}{}}
\newlabel{eq:cmon_errors_gaussian_formula}{{5.48}{204}{To be or not to be Gaussian}{equation.5.3.48}{}}
\newlabel{eq:}{{5.49}{204}{}{equation.5.3.49}{}}
\newlabel{eq:}{{5.50}{204}{}{equation.5.3.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces We show the weak errors computed for the deterministic test, and compare them with the expressions of Eq~\ref  {eq:cmon_errors_gaussian_formula}, which are obtained by assuming that the distribution $P_k(\ell )$ is Gaussian.\relax }}{205}{figure.caption.75}\protected@file@percent }
\newlabel{fig:cmon_damping_err_weak}{{5.8}{205}{We show the weak errors computed for the deterministic test, and compare them with the expressions of Eq~\ref {eq:cmon_errors_gaussian_formula}, which are obtained by assuming that the distribution $P_k(\ell )$ is Gaussian.\relax }{figure.caption.75}{}}
\newlabel{eq:dellgauss}{{5.51}{205}{To be or not to be Gaussian}{equation.5.3.51}{}}
\newlabel{eq:ptau}{{5.52}{205}{To be or not to be Gaussian}{equation.5.3.52}{}}
\newlabel{eq:}{{5.53}{205}{To be or not to be Gaussian}{equation.5.3.53}{}}
\newlabel{eq:}{{5.56}{206}{To be or not to be Gaussian}{equation.5.3.56}{}}
\newlabel{eq:charge_seriess}{{5.57}{206}{To be or not to be Gaussian}{equation.5.3.57}{}}
\newlabel{eq:Pheat}{{5.58}{206}{To be or not to be Gaussian}{equation.5.3.58}{}}
\newlabel{eq:}{{5.60}{207}{To be or not to be Gaussian}{equation.5.3.60}{}}
\newlabel{eq:meanWALDdisto}{{5.61}{207}{To be or not to be Gaussian}{equation.5.3.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Joint system evolution as an OU-process}{207}{subsection.5.3.3}\protected@file@percent }
\newlabel{ssec:oucoupled}{{5.3.3}{207}{Joint system evolution as an OU-process}{subsection.5.3.3}{}}
\citation{gardiner2004handbook}
\abx@aux@cite{0}{gardiner2004handbook}
\abx@aux@segm{0}{0}{gardiner2004handbook}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces We show the stopping time probability distributions, under both hypothesis, obtained numerically by carrying out the SPRT on 20K trajectories for the damping discrimination problem under consideration. Here, we have chosen a value of $a=6$, \textit  {i.e.} $\epsilon \sim 0.0025$.)\relax }}{208}{figure.caption.76}\protected@file@percent }
\newlabel{fig:stop_time_distro}{{5.9}{208}{We show the stopping time probability distributions, under both hypothesis, obtained numerically by carrying out the SPRT on 20K trajectories for the damping discrimination problem under consideration. Here, we have chosen a value of $a=6$, \textit {i.e.} $\epsilon \sim 0.0025$.)\relax }{figure.caption.76}{}}
\citation{sdeint}
\abx@aux@cite{0}{sdeint}
\abx@aux@segm{0}{0}{sdeint}
\citation{robler}
\abx@aux@cite{0}{robler}
\abx@aux@segm{0}{0}{robler}
\newlabel{eq:dlimit}{{5.65}{209}{Joint system evolution as an OU-process}{equation.5.3.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Deterministic vs Sequential}{210}{subsection.5.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Frequency discrimination}{210}{subsubsection*.78}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces We compare the performance of the sequential test against the deterministic one, in terms of the average time that takes the test to reach a certain error threshold. In the deterministic case, we invert the Gaussian model for the symmetric error, to get the desired time. This was done by interpolating the time that the superposition of Lorentzian distributions requires to value the pre-defined error threshold $P_e$, as given when fixing $\epsilon $ in Eq.~\ref  {eq:weakErrorsSPRT} and computing $\frac  {1}{2}(\alpha + \beta )$. Such a Gaussian model is also compared with our numerics, and in this plot we show the numerical value of the symmetric error probability in the deterministic case ($x$ axis), at the different times it was computed for ($y$ axis). For the sequential test, we show the average stopping time as a function of $P_e$, and compare this with the quantity that Wald identity predicts.\relax }}{211}{figure.caption.77}\protected@file@percent }
\newlabel{fig:comparison}{{5.10}{211}{We compare the performance of the sequential test against the deterministic one, in terms of the average time that takes the test to reach a certain error threshold. In the deterministic case, we invert the Gaussian model for the symmetric error, to get the desired time. This was done by interpolating the time that the superposition of Lorentzian distributions requires to value the pre-defined error threshold $P_e$, as given when fixing $\epsilon $ in Eq.~\ref {eq:weakErrorsSPRT} and computing $\frac {1}{2}(\alpha + \beta )$. Such a Gaussian model is also compared with our numerics, and in this plot we show the numerical value of the symmetric error probability in the deterministic case ($x$ axis), at the different times it was computed for ($y$ axis). For the sequential test, we show the average stopping time as a function of $P_e$, and compare this with the quantity that Wald identity predicts.\relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces We show the distributions of the $\ell $ for the frequency discrimination problem, where $\omega _0 = 10^4 Hz$ and $\omega _1 = 1.1 \cdot 10^4 Hz$, $\eta = n = 1$ and $\kappa =1000 Hz$. The reason we take such a high value of $\kappa $ is to aid with the computing resources and time simulation. The results are shown for $10^{4}$ quantum trajectories.\relax }}{212}{figure.caption.79}\protected@file@percent }
\newlabel{fig:freq_histo}{{5.11}{212}{We show the distributions of the $\ell $ for the frequency discrimination problem, where $\omega _0 = 10^4 Hz$ and $\omega _1 = 1.1 \cdot 10^4 Hz$, $\eta = n = 1$ and $\kappa =1000 Hz$. The reason we take such a high value of $\kappa $ is to aid with the computing resources and time simulation. The results are shown for $10^{4}$ quantum trajectories.\relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Here we show the same comparison between the performance of both discrminiation strategies (sequential vs. deterministic), similarly to Fig.~\ref  {fig:comparison}, but for the frequency discrimination case.\relax }}{212}{figure.caption.80}\protected@file@percent }
\newlabel{fig:compa_freq}{{5.12}{212}{Here we show the same comparison between the performance of both discrminiation strategies (sequential vs. deterministic), similarly to Fig.~\ref {fig:comparison}, but for the frequency discrimination case.\relax }{figure.caption.80}{}}
\citation{sindy}
\abx@aux@cite{0}{sindy}
\abx@aux@segm{0}{0}{sindy}
\citation{clerckintroductionnoise}
\abx@aux@cite{0}{clerckintroductionnoise}
\abx@aux@segm{0}{0}{clerckintroductionnoise}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Parameter estimation $\&$ continuously-monitoring}{213}{section.5.4}\protected@file@percent }
\newlabel{sec:cmonEST}{{5.4}{213}{Parameter estimation $\&$ continuously-monitoring}{section.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}The spectral power and the Lorentizan fit}{213}{subsection.5.4.1}\protected@file@percent }
\newlabel{ssec:lorentzian}{{5.4.1}{213}{The spectral power and the Lorentizan fit}{subsection.5.4.1}{}}
\newlabel{eq:}{{5.68}{213}{The spectral power and the Lorentizan fit}{equation.5.4.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces We show the power spectra for a realization of the stochastic process, and the correponding Lorentzian fit.\relax }}{214}{figure.caption.81}\protected@file@percent }
\newlabel{fig:lorentzian}{{5.13}{214}{We show the power spectra for a realization of the stochastic process, and the correponding Lorentzian fit.\relax }{figure.caption.81}{}}
\citation{abadi2016tensorflow}
\abx@aux@cite{0}{abadi2016tensorflow}
\abx@aux@segm{0}{0}{abadi2016tensorflow}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Evolution of the Fisher information}{215}{subsection.5.4.2}\protected@file@percent }
\newlabel{eq:}{{5.69}{215}{Evolution of the Fisher information}{equation.5.4.69}{}}
\newlabel{eq:}{{5.70}{215}{Evolution of the Fisher information}{equation.5.4.70}{}}
\newlabel{eq:}{{5.73}{215}{Evolution of the Fisher information}{equation.5.4.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces We show the evolution for the optimization of the log-likelihood using AD, and an accompanying depiction of the recurrent cell.\relax }}{216}{figure.caption.82}\protected@file@percent }
\newlabel{fig:rncell}{{5.14}{216}{We show the evolution for the optimization of the log-likelihood using AD, and an accompanying depiction of the recurrent cell.\relax }{figure.caption.82}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Maximum-likelihood estimation via automatic-differentiation}{216}{subsection.5.4.3}\protected@file@percent }
\newlabel{eq:}{{5.74}{216}{Maximum-likelihood estimation via automatic-differentiation}{equation.5.4.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces We compare the mean square error of the two methods discussed in the main body with the Cramér-Rao bound.\relax }}{217}{figure.caption.83}\protected@file@percent }
\newlabel{fig:comparison_estimation}{{5.15}{217}{We compare the mean square error of the two methods discussed in the main body with the Cramér-Rao bound.\relax }{figure.caption.83}{}}
\citation{wisemanbook}
\abx@aux@cite{0}{wisemanbook}
\abx@aux@segm{0}{0}{wisemanbook}
\citation{sindy}
\abx@aux@cite{0}{sindy}
\abx@aux@segm{0}{0}{sindy}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}Learning a linear force}{218}{subsection.5.4.4}\protected@file@percent }
\newlabel{eq:}{{5.75}{218}{Learning a linear force}{equation.5.4.75}{}}
\citation{Vargas2021quantum}
\abx@aux@cite{0}{Vargas2021quantum}
\abx@aux@segm{0}{0}{Vargas2021quantum}
\citation{Li2022seq}
\abx@aux@cite{0}{Li2022seq}
\abx@aux@segm{0}{0}{Li2022seq}
\newlabel{fig:forces1}{{5.16a}{219}{\relax }{figure.caption.84}{}}
\newlabel{sub@fig:forces1}{{a}{219}{\relax }{figure.caption.84}{}}
\newlabel{fig:forces2}{{5.16b}{219}{\relax }{figure.caption.84}{}}
\newlabel{sub@fig:forces2}{{b}{219}{\relax }{figure.caption.84}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces We show the results of external force estimation. While the left panel consists on estimating a constant force, the right panel shows that the method learns both the amplitude and decay rate of an exponentially decaying external signal.\relax }}{219}{figure.caption.84}\protected@file@percent }
\newlabel{fig:forces_estimation_cmon}{{5.16}{219}{We show the results of external force estimation. While the left panel consists on estimating a constant force, the right panel shows that the method learns both the amplitude and decay rate of an exponentially decaying external signal.\relax }{figure.caption.84}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Discussion $\&$ future perspectives}{219}{section.5.5}\protected@file@percent }
\citation{cdisc}
\abx@aux@cite{0}{cdisc}
\abx@aux@segm{0}{0}{cdisc}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Code}{220}{subsection.5.5.1}\protected@file@percent }
\newlabel{TotPages}{{244}{220}{}{page.220}{}}
\abx@aux@read@bbl@mdfivesum{nobblfile}
\gdef \@abspage@last{244}
