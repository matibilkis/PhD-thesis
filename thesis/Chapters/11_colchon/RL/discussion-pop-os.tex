In this section we have introduced some basic concepts in reinforcement learning, which can readily be used by an agnostic agent in order to learn \textit{good} behaviours when acting in unknown environments. The goodness of such behaviours is formally quantified by the amount of reward that the agent gets, on average, when departing from its initial state (or the average over possible states, if there is a non-vanishing probability of the interaction being initialized in more than one state). For this, we have introduced value functions, which are the expected return (\textit{i.e.} a weighted sum of stochastic rewards obtained during an episode) that the agent enjoys when following policy $\pi$, and by doing so we have defined an order relation among policies, \textit{i.e.} a policy $\pi^{'}$ is better than a policy $\pi$ if $v_\pi(s) < v_{\pi^{'}}(s)$ $\forall s\in\cS$. In particular, the agent should seek to perform optimally, and that guarantees her to attain as much rewards as possible, on average, during an episode. We have discussed some methods that the agent can used in order to find such optimal policies, through the optimal state value functions, in Sec.~\ref{ssec:1_rl_dp}. Nonetheless, such methods can only work if a perfect model of the environment dynamics is available to the agent, that is, if the agent knows excatly which is the probability of enjoying a reward $r$, and observe its environment transitioning to state $s'$, when departing from state $s$ and performing action $a$. To gain some intuition on the complexity that model-free scenarios carry on, we have stepped back and discussed a simplified setting in Sec.~\ref{ssec:1_rl_bandit}, known as the multi-armed bandit problem, in which we introduced several strategies that balance between exploring how good untaken actions are, and exploiting potentially sub-optimal yet high-reward-retrieving actions; in particular we studied
$\varepsilon$-greedy, UCB and TS strategies. We then moved to the general MDP case in Sec.~\ref{ssec:1_rl_seqDec}, where the usefulness of the state-action value functions $Q(s,a)$ was highlighted. Finally, we introduced an algorithm that allows the agent to learn optimal policies, called Q-learning, in Sec.~\ref{ssec:1_rl_qlearning}.

These concepts and methods will find use in Ch.~\ref{chapter:RLCOH}, where a reinforcement learning agent is asked to calibrate a quantum receiver out of several repetitions of the experiment. The model-free features of the algorithms we have discussed can readily be used to deal with scenarios where quantum information is transmitted over unknown quantum channels, a result we will present in Sec.~\ref{ssec:rlcoh_noise}. Let us stress that here, even finding sub-optimal strategies might already be a step forward towards better understanding the structure of quantum communication protocols, since little is known about such systems. In this regard, there is plenty of room for the type of machine learning heuristics that are presented along this thesis, in order to help with the development of quantum information protocols. %Thus, even if the reinforcement learning agent gets stuck in sub-optimal policies
