The complexity added in MDPs, as compared to bandit problems (MRPs), is that agent and environment sequentially interact during (a possibly infinite number of) time-steps. Similar to the bandit case, agent's objective is to acquire as much reward as possible during an episode, and as a matter of fact this strongly depends on the policy that the agent follows. At the end of episode $t$, in which a sequence of tuples %of states, actions, rewards and next states
$\llaves{(s_\ell, a_\ell, r_{\ell+1})}_{\ell = 0}^{L}$ has been experienced (with $s_{L+1}$ a \textit{terminal state}, and $L$ generally varying among different episodes), the agent's performance after each time-step $\ell$ is evaluated using the so-called \textit{return},
\begin{equation} \label{eq:returnGT}
G_{\ell}^{(t)}=\sum_{i=0}^{L-\ell}\gamma^i r_{i+\ell+1}^{(t)},
\end{equation}
which is the weighted sum of rewards obtained at all future time-steps, with a \textit{discount factor} $\gamma\in(0,1]$, weighting more the rewards that are closer in the future. Note that for infinite-horizon MDPs, i.e., $L\rightarrow\infty$, it must hold $\gamma<1$ to ensure that $G_\ell$ remains finite.

By introducing the return, it is straightforward to assign a value to a state $s$ for a given interaction policy $\pi$, via the so-called \textit{state value function}:
% By following interaction policy $\pi$, the value function of state $s$ is defined as
\begin{equation}\label{eq:vFunc}
v_\pi(s)=\mathbb{E}{\pi}[{G_\ell | s_\ell=s}],
\end{equation}
which is the expected return over all possible trajectories that start from state $s$, take actions according to policy $\pi$ and whose dynamics is governed by $\tau$. In other words, the value function measures how convenient it is to visit state $s$ when policy $\pi$ is being followed, and thus provides a route to qualify the goodness of such policy. Note that this quantity is completely determined by the future trajectories accessible from $s$ and hence its dependence on the time-step $\ell$ can have at most the effect of restricting the set of states on which $v_{\pi}(s)$ is supported at that time; we keep this dependence implicit unless otherwise stated. By writing explicitly the expected value for the first future time-step in Eq.~\eqref{eq:vFunc} and then applying the definition of $v$ recursively, it is easy to show that the state-value function satisfies, for any policy, the \textit{Bellman equation}~\cite{Bellman2003}:
\begin{equation}\label{eq:vBell}
v_\pi(s) = \sum_{\substack{ s'\in\cS, r\in\cR\,\\ a\in\cA}}\tau(s',r|s,a)\pi(a|s)\left(r+\gamma v_\pi(s')\right).
\end{equation}
This equation relates the value of a state $s$ with that of its nearest neighbours $s'$, which can be reached with a single action from $s$, and with the corresponding reward obtained by performing such action.

Since value functions assign a score to the policy $\pi$ being followed, a possible route to solve the reinforcement learning problem is that of maximizing state-value functions. Specifically, the optimal policy $\pi^{*}$, maximizes the state-value function for each $s$ and thus represents the \textit{optimal value function}, which satisfies a particular Bellman equation, which is known as the \textit{optimal Bellman equation}:
\begin{align}\label{eq:vBellOp}
v^{*}(s)&:=v_{\pi^*}(s)=\max_\pi v_\pi(s)\\
&=\max_{a\in\cA}\sum_{s'\in\cS,r\in\cR}\tau(s',r|s,a)\left(r+\gamma v^{*}(s')\right).
\end{align}
In an hypothetical situation where optimal value functions are available, an optimal policy can readily be constructed by selecting an action that, from each state, takes the environment towards the next state whose state-value function is the highest. Nevertheless, to do so one needs to have a precise mapping between actions and next-states, an information which is missing if environment dynamics $\tau(s'|s,a)$ is unavailable to the agent.

For this reason, we define the state-action value function (or Q-function, or Q-value) as the expected return when starting from state $s$ and performing action $a$:
\begin{equation}
Q_\pi(s,a) = \mathbb{E}{\pi}\left[G_\ell | s_\ell=s, a_\ell=a\right],
\end{equation}
which is related to the state-value function by $v_\pi(s)=\sum_{a\in\cA} \pi(a|s) Q_\pi(s,a)$. Whenever it is clear from the context we will drop the dependence on the policy $\pi$ for $Q(s,a)$.
Similar to the state value function, by writing explicitly the expected value for the first future time-step, the Bellman equation for state-action value function reads:
\begin{eqnarray}\label{eq:bellqas}
Q_{\pi}(s,a)&=& \sum_{\substack{ s'\in\cS, r\in\cR\,\\ a\in\cA} } \tau(s',r|s,a)(r+\gamma \pi(a'|s')Q_{\pi}(s',a')).\nonumber
\end{eqnarray}

Importantly, the optimal policy $\pi^*$ can also be obtained by maximizing the Q-function, with a corresponding optimal Bellman equation
\begin{eqnarray}\label{eq:qBellOp}
Q^{*}(s,a)&&:=Q_{\pi^*}(s,a)=\max_\pi Q_\pi(s,a)\\
&&=\sum_{s'\in\cS,r\in\cR}\tau(s',r|s,a)(r+\gamma \max_{a'\in\cA}Q^{*}(s',a')).\nonumber
\end{eqnarray}
Note that the state-action value function $Q(s,a)$ provides a natural generalization of the expected reward $Q(a)$ to the case where agent and environment interact sequentially during an episode, where the necessity of defining enivornment states arises. In the latter case, the Bellman equation trivially reduces to only the first term in the right-hand side of Eq.~\ref{eq:vBell}.

We are now in position to understand the complexity of the reinforcement learning problem. While agent and environment begin to interact through some policy $\pi$, state-action value functions are not available to the agent. In turn, several repetitions of such interaction are required in order to  asses which is the relevant subspace of states and actions (recall that environment dynamics $\tau(s',r|s,a)$ might be stochastic). Since the agent can not do better than randomly sampling actions at the beggining of the learning process, she needs to wait a transient time --- which is related to the probability of randomly arriving to a high-reward region of the state-action space --- until some reward signal that might guide the search is experienced. Once obtained a reward signal, a balance between exploring new regions of the state-action space, and to better estimate current state-action value functions should be made. Here, even if an action deterministically maps a state $s$ to a state $s'$, the agent needs several episodes in order to estimate value functions $Q_\pi(s,a)$, since those depend on the accessible region of the state-action space under policy $\pi$, once $s'$ has been reached, as explicited in the Bellman equation of Eq.~\ref{eq:bellqas}. Moreover, such policy should be modified in order to maximize the expected return. Overall, reaching a reasonable balance between exploration and exploitation in such scenarios is certainly challenging, particularly if the agent has resource constraints such as a finite number of episodes available in order to learn a reasonably good policy.

In this regard, while the model-free assumption is strong, it often happens that such agents discover unknown protocols solving the probem at hand. While the solutions found by RL might not be optimal, they can be better than any previously known one. Moreover, the versatily of the reinforcement learning setting (in particular when defining states, actions and rewards) constitutes an extremely appealing tool the discovery of ansatz to problems that are challenging to tackle (or even to model), as we do in Chapter~\ref{chapter:RLCOH}.
%The problem we have introduced is certainly a hard one. While the model-free assumption is strong, it is often customary, since modelling environments can also be a hard task. %Despite of the complexities and issues stressed above, the reinforcement learning community has arrived to a series of outstanding results --- probably in record time, considering how much effort has it take Scientific Community to reach desired goals ---.

In what follows we will detail one out of the --- very many --- algorithms that can be used in order to learn optimal policies in model-free, reinforcement learning scenarios.

%
