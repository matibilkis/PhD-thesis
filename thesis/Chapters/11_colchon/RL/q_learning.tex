In the model-free setting, the agent not only has to find an optimal policy by exploiting valuable actions, but also needs to characterize the environment in the first place by exploring possibly advantageous configurations. In such a case, the Q-value is quite helpful since it associates a value to the transitions determined by taking action $a$ from state $s$ and following policy $\pi$ thereafter.

Q-learning is an algorithm that can be used to learn optimal policies in a model-free way, and it was first proposed by Watkins \cite{Watkins1989}. This algorothm is often used as a basis for more advanced RL algorithms~\cite{Mnih2013, ddpg}.
It is based on the observation that any Bellman operator, i.e., the operator describing the evolution of a value function as in Eqs.~(\ref{eq:vBell},\ref{eq:vBellOp},\ref{eq:qBellOp}), is contractive~\cite{algsrl}. This implies that, under repeated applications of a Bellman operator, any value function converges to a fixed point, which by construction satisfies the corresponding Bellman equation. Thus, in order to find $Q^{*}(s,a)$, Q-learning turns the optimal Bellman equation for Q, Eq.~\eqref{eq:qBellOp}, into an update rule for $\hat{Q}(s_{\ell},a_{\ell})$, \textit{i.e.}
, the Q-function's estimate available to the agent at a given time-step $\ell$ of any episode $t=1,\cdots,L$.

After an interaction step $s_{\ell}\rightarrow a_{\ell}\rightarrow r_{\ell+1}\rightarrow s_{\ell+1}$ is experienced, the update rule for the Q-estimate is
\begin{align}\label{eq:QLUPDATERULE}
\hat{Q}(s_{\ell}, a_{\ell}) & \leftarrow (1-\lambda_t(s_{\ell},a_{\ell}))\hat{Q}(s_{\ell}, a_{\ell})\\
&+ \lambda_t(s_{\ell},a_{\ell}) \left(r_{\ell+1}  + \gamma \max_{a'\in\cA(s_{\ell+1})}\hat{Q}(s_{\ell+1}, a')\right),
\end{align}
where $\lambda_t(s,a)$ is the learning rate, which depends on the number of times the state-action pair $(s_{\ell},a_{\ell})$ has been visited.
%In case that the state-action pair $(s_\ell, a_{\ell})$ is visited $k$ times during a single episode, the expected return can be estimated using the \textit{first-visit} or the \textit{every-visit} experiences, where in the second case \eqref{eq:QLUPDATERULE} is applied $k$ times during episode $t$ \cite{Sutton2018}.
Note that in order to do the update at each time-step $\ell$, it is only necessary to enjoy the next immediate reward $r_{\ell+1}$ and observe the next state $s_{\ell+1}$; this method thereby allows an on-line learning of the MDP. A pseudo-code of the algorithm can be found below.
%As it turn to be equivalent for our specific setting, in here all updates are done at the end of the episode.

\begin{algorithm}[h]\label{alg:ql}
  \DontPrintSemicolon
  \SetAlgoNoEnd
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{$\hat{Q}(s,a)$ \texttt{arbitrarly initialized} $\forall s \in \cS, \; \forall a \in \cA (s)$; \texttt{learning rates} $\lambda_t(s_\ell,a_\ell) \in (0, 1]$, $\epsilon > 0$}
  \Output{$\hat{Q}(s,a) \sim Q^{*}(s,a)$}\;
  \For{ $t$ in $1$ ... $T$  }{
  $\; \; \;$ \texttt{initialize} $ s_0  \; \; \;$ \;
  $\; \; \;$ \For{\texttt{step }$\ell$ \texttt{in episode} $t$}{$\; \; \; \; \; \;$\texttt{take action }$a_\ell$ \texttt{according to }$\pi$ (e.g. $\epsilon$\texttt{-greedy})\; $\; \; \; \; \; \;$\texttt{observe reward }$r_{\ell+1}$\texttt{ and next state }$s_{\ell+1}$ \; $\; \; \; \; \; \;  $\texttt{update }$\hat{Q}(s_\ell, a_\ell)$ \texttt{according to: }\; $ \; \; \; \;  \; \; \; \; \; \; \hat{Q}(s_{\ell}, a_{\ell}) \leftarrow \hat{Q}(s_{\ell}, a_{\ell})  + \lambda (s_\ell, a_{\ell}) [r_{\ell+1} +$\; $\; \; \; \; \; \;  \; \; \; \;  \gamma \max_{a'} \hat{Q}(s_{\ell+1}, a') - \hat{Q}(s_{\ell}, a_{\ell}) ]$\;$\; \; \; \; \; \;$\If{$s_{\ell+1}$\texttt{ is terminal state}}{$\; \; \; \; \; \; \; \; \; \;$ break}$\; \; \; \; \; \;$\Else{}{$\; \; \; \; \; \; \; \; \; \;s_{\ell} \leftarrow s_{\ell+1}$}
  }
}
\caption{Q-learning pseudo-code.}
\end{algorithm}

After a large number $n$ of iterations of the update rule Eq. \eqref{eq:QLUPDATERULE} for all state-action couples, the convergence of the Q-estimate to the optimal Q-function is guaranteed by two general conditions on the learning rate (also known as Robinson conditions)~\cite{Watkins1989,Sutton2018}:
\begin{align}\label{eq:qLConv}
\hat{Q}(s,a)&\underset{k\rightarrow\infty}{\rightarrow}Q^{*}(s,a)\;\,\forall s\in\cS, a\in\cA(s)\\
&\text{ iff }\sum_{t(s,a)}\lambda_{t}(s,a)=\infty,\;\sum_{t(s,a)}\lambda_{t}(s,a)^{2}<\infty,
\end{align}
where the sums are taken over all interactions at which a given state-action couple is visited.
Once the optimal Q-function is obtained, an optimal (deterministic) policy can be constructed by ``going greedy'' with respect to it, i.e., $\pi^{*}(a|s) = \delta(a,\argmax{a\in\cA} Q^{*}(s,a))$ for all $s\in\cS$, where $\delta(x,y)$ is a Kronecker delta.

In RL literatue, Q-learning is classified as an off-policy method~\cite{Sutton2018}, meaning that it learns the state-action values of a \textit{target policy} - in this case the optimal policy - by taking actions according to an \textit{interaction policy}, generally differing from the first one. The standard Q-learning method commits to an $\epsilon$-greedy interaction policy, where with probability $\epsilon$ the agent chooses a random action and otherwise it chooses the greedy action that maximizes the current Q-estimate. However, as we will see in Sec.~\ref{ssec:rlcoh_dolinar_plus_bandit}, more general strategies can be considered, including a clever search as inspired by UCB or TS algorithms.%, where the policy

Moreover, Q-learning is also classified as a tabular method~\cite{Sutton2018}, meaning that it does not approximate or infer the value function of state-actions that have not been visited. In the latter case, it is customary to use an artificial neural network in order to infer the value of the Q-function, and much of the recent success of reinforcement learning algorithms in discovering new protocols for a wide variaty of problems hinges on such networks.

There is a plethora of reinforcement learning algorithms that has been proposed according to specific constraints imposed in the setting, and there is generally no best algorithm; on the contrary --- as it happens in real-life --- the success of an idea generally depends on the context it is carried out. In general, a combination of them needs to be done in order to attain succesful learning and to prove an advantage over state-of-the-art techniques, when tackling a problem from the reinforcement learning perspective. Before closing up this section in reinforcement learning, let us comment on the case where a model of environment dynamics is available to the agent.
%%%
