In the model-aware scenario, the agent has access to the environment dynamics $\tau(s'r | s,a)$. Nonetheless, the question of how to design an optimal policy still holds: while estimating value functions out of samples is no longer required, the agent still needs to solve the Bellman equation in Eq.~\ref{eq:vBell} in order to compute $v_\pi(s)$ for $s\in\cS$. Moreover, the policy optimization challenge remains, and solving it can be highly non-trivial. Such a problem is known as planning~\cite{Sutton2018}, and can be solved for finite-horizon MDPs via dynamic programming methods.

Here we distinguish between two approaches. On the one hand, there is the sequential approach, which exploits the structure of Bellman equation and solves the policy optimization by splitting the global policy optimization into a sequence of smaller optimization problems~\cite{Bellman2003}. On the other hand, iterative approaches can be formulated for the policy optimization problem, such as value and policy iteration algorithms. The latter approach exploits the contractive property of state value function, and are sligthly more general than the methods we consider here, since they can readily be applied in scenarios where  $L$ (the number of time-steps in each episode) is not fixed~\cite{Sutton2018}). In particular, the state value function can be seen as a fixed point of a contractive operator, called the Bellman operator~\cite{AlgorithmsRLCsaba}. While policy iteration algorithms consist on a loop where value functions are estimated through such contractiveness property and a policy is modified in such a way to optimize such values, value iteration algorihtms straightfowardly apply the optimal Bellman operator in order to compute $v^*(s)\; \forall s\in\cS$; more insight can be gained in \href{https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html}{this online demonstration}.

In this thesis we restrict to a sequential optimization of the state value, since we will apply RL algorithms to a problem with a well-defined,  sequential structure, namely the calibration of Dolinar receivers. We thus follow the method introduced by Bellman~\cite{Bellman2003}, which makes use of the recursive relation of Eq.~\eqref{eq:vBellOp} to find the optimal policy step by step, and for this we assume that every episode deterministically ends at a fixed time-step $L$, and denote by $v^{*}_{\ell}(s)$ the optimal value function of state $s$ at time-step $\ell$. Here, the so-called \textit{principle of optimality} holds, which is nothing than the optimal value equation in Eq.~\ref{eq:qBellOp}, and states that an optimal policy can be obtained by going greedy sequentially, \textit{e.g.} step by step. Hence, since the optimal policy consists in taking the best possible action from any given state, it can be constructed by concatenating optimal (and local) policies at each time-step: we start by solving Eq.~\eqref{eq:vBellOp} at the last time-step,
\begin{equation}\label{eq:dpL}
v^{*}_{L}(s)=\max_{a\in\cA(s_{L})}\sum_{r\in\cR}\tau(r|s,a)r,
\end{equation}
where we used the fact that $v_{L+1}(s)=0$. The solution to Eq.~\eqref{eq:dpL} provides the optimal action at step $L-1$ for each $s$ and the optimal value function $v^{*}_{L}(s)$.
Then we plug the latter into the optimal Bellman equation for the previous time-step, which in turn can be solved to obtain the optimal action and value function $v^{*}_{L-1}(s)$ for every $s$. By repeating this procedure iteratively for each time-step $\ell=L,\cdots,0$, we can obtain the optimal sequence of actions and value functions for any state at any time-step. This an approach has the nice interpretation of constructing the solution of a large problem by solving smaller problems, each stage essentially adding some complexity into the problem at hand.

The difficulty in this method is that the optimization over actions at a given state \textit{i.e.} $\max_{a\in\cA(s_{\ell})}$ might be find problems. As a matter of fact, we will find difficulties in such optimization landscape when calibrating coherent-state receivers in the presence of noisy channels (see Sec.~\ref{ssec:rlcoh_noise}.)
