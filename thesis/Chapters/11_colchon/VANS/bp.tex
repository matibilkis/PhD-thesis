The barren plateau (BP) phenomenon has recently received considerable attention as one of the main challenges to overcome for the VQA framework to provide a quantum speedup. BPs refer to the exponentially vanishing (in the number of qubits) for a random circuit to present a gradient component which is slightly higher than zero. In turn, this constitutes a big challenge when navigating the cost-function landscape, since such landscape (\textit{e.g.} the set of unitaries that are generated when varying the parameters of a sufficiently random PQC) becomes flat in the pressence of a BP.

In the following we will first outline how the BP phenomenon emerges from a 2-design, and then discuss several extensions of this phenomena to other classes of circuits.

\vspace{1cm}

The notion of $t$-design quantifies how similar the measure obtained from varying parameters $\thv$ in $U(\kvec, \thv)$ differs from that of the Haar measure, which is the uniform measure in $U(n)$. A random event refers to randomly sampling an unitary transformation from $\mathcal{U}$, \textit{i.e.} the set of unitaries that are generated from $U(\kvec, \thv)$ by modifying the parameters $\thv$. Thus, we can define an expressibility super-operator as
\equ{\mathcal{A}^{(t)}_\mathcal{U}(\cdot) = \int_{\mathcal{U}} d_\mu(U)\; U^{\otimes t} (\cdot) {U^\dagger}^{\otimes t} - \int_{U(n)} d_{\mu_H}(U) \;U^{\otimes t} (\cdot) {U^\dagger}^{\otimes t},}
where $d_{\mu_H}(U)$ denotes the volume element of the Haar measure, and $d_\mu(U)$ is the volume element corresponding to the uniform distribution over $\mathcal{U}$~\cite{holmes2021connecting}; for PQCs such set is generated by uniformly sampling over the parameters $\thv$.

Thus, $\mathcal{U}$ forms a $t$-design if $\mathcal{A}^{(i)}(X) = 0$ for every operator $X$ and every $i=1,...,t$, meaning that the uniform distribution over $\mathcal{U}$ matches the uniform distribution over $U(n)$ up to the first $t$-moments. Using some useful identities from integrating over the Haar measure, namely that
\begin{align}\label{eq:relations_haar}
\int_{U(n)} d_\mu(U)Â \; U_{ij} U^{*}_{mk} =& \frac{\delta_{im}\delta_{jk}}{n} \\
\int_{U(n)} d_\mu(U) \; U_{ij} U_{kl} U^*_{mn} U^*_{op}  =& \frac{1}{n^2-1}(\delta_{im} \delta_{jn} \delta_{ko} \delta_{np} + \delta_{io} \delta_{km} \delta_{jp} \delta_{ln}) \\ & -\frac{1}{n(n^2-1)}(\delta_{i m} \delta_{ko} \delta_{jp} \delta_{ln} + \delta_{io} \delta_{km} \delta_{jn} \delta_{lp}), \nonumber
\end{align}
the landscape of $2$-designs was explored in Ref.~\cite{mcclean2018barren}. In particular, using the identities we can show that 2-designs have gradients which are on average (over the Haar measure) zero-valued, which indicate that the landscape is not \textit{biased} towards any particular direction. Moreover, the identities can also be used to compute the variance of the cost-function gradient, and the result (for $2$-designs) is that it exponentially vanishes with the number of qubits present in the circuit:
\begin{equation}\label{eq:BP}
    \Var\left[\partial _\alpha C_{\text{VQE}}(\kvec,\thv)\right]\leq F(n)\,, \quad \text{with} \quad F(n)= \OC\left(\frac{1}{2^n}\right)\ ,
\end{equation}
where $\alpha\in\thv$. From Chebyshev's inequality we have that $\Var\left[\partial _{\alpha} C(\kvec,\thv)\right]$ bounds the probability that the cost-function partial derivative deviates from its mean value (of zero) as
\begin{equation}\label{eq:Chebyshev}
    \Pr\left[\left|\partial_\alpha C(\kvec,\vec{\theta})\right|\geq c\right]\leq\frac{\Var[\partial_\alpha C(\kvec,\vec{\theta})]}{c^2} \sim \mathcal{O}(2^{-n}),\,,
\end{equation}
for any $c>0$. This indicates that when navigating the set $\mathcal{U}$ by varying parameters $\thv$, an optimizer will observe a considerably flat landscape with a high probability. Here, such probability is associated to the chances that, after a random circuit initialization, the cost-function value associated to such circuit presents a gradient whose value is larger than $c$, as per Eq.~\ref{eq:Chebyshev}.

Thus, two difficulties arise when using a quantum computer in this context.

On the one hand, we will need to deal with shot-noise arising from meaurements done to estimate the cost-function value. Here, the accuracy of the estimations is proportional to the squared-root-inverse of the number of shots $N$.

On the other hand, navigating through the set $\mathcal{U}$, \textit{i.e.} the set of unitary transformations that can be reached from the parametrized quantum circuit under consideration $U(\kvec,\thv)$, when varying the continuous parameters $\thv$. Here, the probability of reaching a higher-than-$\epsilon$ gradient in Eq.~\ref{eq:Chebyshev} should be understood in terms of the parameter landscape: high gradients are exponentially unlikely to be reached, when randomly varying the parameters. Nevertheless, the optimizer will ultimately need to deal with such small gradients, and for the optimization procedure not to behave like a random-walk, we will need to resolve between this gradients, and the accuraccy required to accomplish that will generally require a high number of measurements~\cite{mcclean2018barren}. In this sense, exponentially many resources will be required to navigate such flat landscape.


While barren plateaus were first identified for large circuits, they were shown to be present in shallow circuits as well. Here, cost-function locality plays a crucial role: while several choices of observables $\{O_i\}$ and functions $\{f_i\}$ can lead to different faithful cost functions (\textit{i.e.}, cost functions whose global optima correspond to the solution of the problem), global cost functions can lead to trainability issues for large problem sizes~\cite{cerezo2020cost,sharma2020trainability}. Here we recall that global cost functions are defined as ones where $O_i$ acts non-trivially on all $n$ qubits. On a different note, the BP-phenomena discussed above has so-far considered 2-designs. In this sense, Refs.~\cite{holmes2021connecting,larocca2021diagnosing} study the presence of BPs when the circuits are $\epsilon$-close to a 2-design. In particular, the expressibility of a quantum circuit (\textit{i.e.}, which sample large regions of the unitary group~\cite{sukin2019expressibility}), can be linked to the amount of entanglement it generates~\cite{sharma2020trainability,patti2020entanglement,marrero2020entanglement}, which serves as a tool to diagnosticate the presence of a BP. For example, the Hardware-Efficient-Ansatz is a highly expressible circuit, and consequently suffers from an expressibility-induced BP. Several promising strategies have been proposed to \textit{mitigate barren plateaus}, such as correlating parameters~\cite{volkoff2021large}, layerwise training~\cite{skolik2020layerwise}, and clever parameter initialization~\cite{grant2019initialization,verdon2019learning}. Nonetheless, constructing smart ansatzes that do not present BPs seem to be the most promising route to avoid them; an example of such are the Quantum Convolutional Neural Networks~\cite{Cong2019,pesah2020absence}.

However, this is not the end of the barren-pleateu story: there exists a second effect that leads to barren plateaus which can even affect smart ansatzes with no randomness or entanglement-induced barren plateaus. As shown in Ref.~\cite{wang2020noise}, the presence of certain noise models acting throughout the circuit maps the input state toward the fixed point of the noise model (\textit{i.e.}, the maximally mixed state)~\cite{wang2020noise,franca2020limitations}, which effectively implies that the cost function value concentrates exponentially around its average as the circuit depth increases. Explicitly, in a noise-induced barren plateau (NIBP) we now find that
\begin{equation}\label{eq:NIBP}
    \left|\partial _{\alpha} C(\kvec,\thv)\right|\leq g(n)\,, \quad \text{with} \quad g(L)= \OC\left(\frac{1}{q^L}\right)\,,
\end{equation}
where $q>1$ is a noise parameter and $L$ the number of ansatz's layers. From Eq.~\ref{eq:NIBP} we see that noise-induced barren plateaus will be critical for circuits whose depth scales (at least linearly) with the number of qubits. It is worth remarking that Eq.~\ref{eq:NIBP} is no longer probabilistic as the whole landscape flattens. Here, we note that strategies aimed at reducing the expressibility of the circuit cannot generally prevent the cost from having a noise-induced barren plateau, since here reducing the circuit noise (\textit{e.g.} improving the quantum hardware) and employing shallow circuits seem to be the only viable and promising strategies to prevent these barren plateaus at the moment.
