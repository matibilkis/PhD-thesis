This Section represents the main contribution of the Chapter, namely the real-time model-free calibration of coherent-state receievers.

Here, the reinforcement-learning agent is asked to optimally-calibrate a $L$-Dolinar receiver, but is provided  with essentially no information about the setting: she can only select actions according to the experience gathered via sequential repetitions of the discrimination experiment.

By the end of each episode (each discrimination experiment), the agent is only given \textit{one bit of information}, \textit{i.e.} the reward that accounts for the correcteness of her guess. Thus, if the agent is able to calibrate the receiver such that it performs near-optimal actions $\llaves{a_\ell}$, then the rewards enjoyed will be higher on average. This stresses the difficulty of the setting proposed: \textit{even if the receiver was calibrated to perform optimally, there is a chance of enjoying no reward}. In turn, the reward is a Bernoulli-distributed random variable with success probabilty $P_s^{L}$, and even the optimal configuration in Eq.~\ref{eq:32OptSuc} will experience a zero reward with probability $1-P_*^{L}$.

% The commented in the introduction of this Chapter, studying the performance of finite Dolinar receivrers (\textit{e.g.} finite $L$) is experimentally justified, whereas the model-free assumption is motivated by communication scenarios where the presence of unknown quantum channels can non-trivially act on the transmitted signal. Thus, our model-free approach finds its roots in Reinforcement Learning, which was discussed previously; in order to apply such ideas to the calibration of this receiver, we will require some definitions, which are given in the following Section.
