Reinforcement learning was discussed in Sec.~\ref{sec:1_rl}, where the versatility of such framework was stressed. In particular, once a problem of interest is translated into the RL formalism, algorithms such as Q-learning (introduced in Sec.~\ref{ssec:1_rl_qlearning}) can readily be applied.%, in od to tackle it in a model-free manner.

For this purpose, we now need to specify the corresponding definitions of states, actions, rewards and episodes specific in our problem of calibrating the $L$-Dolinar receiver. Hence, we now state:
\begin{itemize}
\item Each episode $t$ corresponds to an independent discrimination experiment, with a new default state $s_{0}=\alpha^{(k)}$ sampled from $p_{k}$, $k\in\{0,1\}$
\item Each episode consists of $L+1$ time-step $\ell=0,\cdots,L$, corresponding to the $L$ detection layers followed by the final guessing stage;
\item The possible states of the environment at time-steps $\ell$ are $s_{\ell}=\alpha^{(k)}_{\ell}$, i.e., the transmitted part of $s_{0}$ at that layer;
\item The agent is not aware of the state $s_{\ell}$, in particular it does not know which hypothesis is true, but it can observe the measurement outcome $o_{\ell}$, $0 < \ell \leq L$;
\item The actions $a_\ell$ available at time-step $0 \leq \ell <L$ are the displacements $\beta_{\ell}$ available at that layer, conditioned on the history of observations and actions $h_{\ell}=(a_0, o_1, ...,a_{\ell-1},o_\ell)$, while at the last step they constitute the guess, $a(h_{L})=\hat{k}\in\{0,1\}$. For a given state $s_\ell$, the set of available actions is denoted as $\mathcal{A}(s_{\ell})$.
\item The reward $r\in\{0,1\}$ is non-zero only at the end of the episode and provided that the guess is correct, hence the transition function for the environment $\tau(r,s'|s,a)$ is
\begin{align}\label{eq:sTr}
&\tau(\alpha^{(k')}_{\ell+1}|\alpha^{(k)}_\ell,a_{\ell})=\delta(k',k)\;\;\forall\ell\leq L,\\
&\tau(r_{L+1}|\alpha^{(k)}_L,a_L)=\delta(r_{L+1},1)\delta(a_L,k),
\end{align}
were we omitted the trivial reward for $\ell\leq L$.
\item Measurement outcomes obtained from the $\ell$-th photodetector will be denoted as $o_\ell$. The set of possible observations is denoted as $\mathcal{O}$.
\item A \textit{policy} will correspond to the entire set of conditional actions and guess $\llaves{a_\ell}$, which can be thought as a pre-defined receiver configuration. In particular, we target to find the \textit{optimal} configuration, \textit{i.e.} that leading to the highest expected reward.
\end{itemize}
Additionally we set $\gamma=1$ since the process has a finite horizon ($L$); recall that such parameter acts as a regularizer for the \textit{return} function in infinite-horizon problems (see, \textit{e.g.} Eq.~\ref{eq:returnGT}).

Similarly to the model-aware situation studied in Sec.~\ref{sec:rlcoh_model_aware_approach_intro}, we need to specify agent's reconstruction of environment state (recall we are dealing with a Partially Obersvable Markov Decision Process, and as such the state of the environment is not fully observable by the agent at each time-step). While when applying dynammic programming techniques we defined agent's state to be the belief of $b_\ell$ of having $\ket{\alpha}$, the model-free agent is not able to perform the bayesian update, since it ignores outcomes probabilities distributions.

Thus, we define agent's state as the history $h_\ell$ of observations and actions made up to the $\ell$-th stage. An important consequence of such definitions is that the expected return, departing from the initial state (that is, the state value function of $s_0$, independent of agent awareness) is the success probability associated to agent's policy.

In contrast to the dynamic programming approach, the agent needs now to estimate the state-value functions out of several repetitions of the experiment, in order to asses how valuable a given configuraton is.

In particular, in Sec.~\ref{ssec:optVal} we will show that the optimal state-action value functions are in close correspondence to the ultimate attainable success probability $P_*^{L}$. Thus, if the agent is able to find such state-action value functions, then it can readily construct the optimal policy, as described in Sec.~\ref{ssec:1_rl_seqDec}. To do so, we now bring the Q-learning algorithm into the play.
